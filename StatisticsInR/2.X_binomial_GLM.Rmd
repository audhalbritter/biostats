---
title: "Binomial generalised linear models"
output:
  bookdown::html_document2:
    highlight: tango
    toc: true
    toc_float: true
    css: ../css/style-chapters.css
---

```{r setup, eval=TRUE, include=FALSE}

# add all packages that need loading
library(tidyverse)
library(patchwork)
library(knitr)
#library(readr)
#library(GGally)
#library(ggpubr)

# source figure settings
source("../StatisticsInR/Files/biostats_theme.R")

# set warning to be off for all chunks
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

```

# Binomial GLM

```{r setup-data, include=FALSE, echo=FALSE}

# load the sheep data
sheep_data <- read_csv("../StatisticsInR/Files/sheep_data.csv")

sheep_data <- sheep_data[is.na(sheep_data$Age)==FALSE,]

# create some example data
set.seed(2020)
your_data <- example_data <- tibble(explanatory = c(rnorm(50, 5, 3), 
                                                    rnorm(50, 10, 3)),
                           response = (c(sample(0:1, 50, replace = TRUE, 
                                      prob = c(0.8, 0.2)), 
                                sample(0:1, 50, replace = TRUE,
                                       prob = c(0.2, 0.8)))))

```

### Before you start {.facta .toc-ignore}

Before reading this page, you should be comfortable with basic statistical 
theory, using R, continuous and categorical data, and **linear models**.

</br>

## Introduction

In this section, we will look at how we can use a Binomial Generalized Linear 
Model (GLM) 
to analyze data with a binary response and numeric explanatory variables.

This model builds on from the standard linear models covered on these pages 
[1](), [2](), [3](). GLMs are similar to the linear models conceptually and in 
R, but are very much more flexible.

Like the linear regression, the GLM also has two motivations. 
**inference** and **prediction**. But as the name suggests, it is more general
than the standard linear models (used for linear regression, ANOVA, and analyses
with categorical and continuous variables). 

</br>

Linear models are used to model a continuous response as a function of 
explanatory variables. 
GLMs also model a response as a function of explanatory 
variables. However, as they are more flexible 
GLMs can
be used for discrete as well as continuous response variables, They can model
non-linear relationships, and handle cases where model residuals would not be
normally distributed. 

</br>

In particular, GLMs are useful when the assumptions of the linear model are 
violated. The most common violations that can be addressed with a GLM are:

 * Residuals that are not normally distributed
 * Non-linearity
 * Unequal variance 
 
While some of these violations could be addressed by transformation of the 
response to try and improve linearity or equalise the variance - 
this is not always possible or preferable. 
The GLM makes it possible to account for violations of
linearity and variance of residuals in a single model without changing the 
response. This is especially useful when you know that the response data will
not follow a normal distribution e.g. if they are binary results or derive from
counting. In these cases, different distributions will better represent the data
generation process than the normal distribution used in the linear model.

</br>

On this page, we focus on one particular type of GLM, the Binomial GLM. The
Binomial GLM fits a non-linear line to:

1) estimate a relationship between $X$ and $Y$, where $Y$ is binary and 
therefore bounded between 0 and 1. 

2) predict change in $Y$ from change in $X$. 

</br>

But unlike a linear model, the Binomial GLM does not do this with a straight
line (on the scale of the data). The Binomial GLM fits a curved line bounded
between 0 and 1 on the y-axis. This is because the Binomial GLM (also called a 
logistic regression - more on why later), 
the response data are always binary. The $Y$ values can only take either 0 or 1, 
Yes or No, etc, anything with only two outcomes.  

```{r nonlinear-plot, include = TRUE, echo = FALSE, , fig.cap = "Example of a fitted Binomial GLM model. The estimated relationship is plotted on the scale of the data."}

ggplot(aes(x = explanatory, y = response), data = example_data)+
  geom_point(shape = 16)+
  geom_smooth(method=stats::glm, method.args = list(family="binomial"), 
              se=TRUE)

```

</br>

A large number of the models that are used in biological research are GLMs. 
This
is because a lot of biological data would not meet the assumptions required for
a linear model, for example survival data, occupancy data, or presence of a 
particular gene are all examples of binary responses. 

As a result of their wide usage, 
GLMs are a key part of modern quantitative biology!

</br>

## <i class="far fa-question-circle"></i> Which questions? 

Example questions that can be answered with the Binomial GLM:

Inference

 * How does body weight (kg) influence survival probability of sparrows?
 * How does forest cover (type) affect the occurrence probability
 of a plant species?

Prediction

 * What is the mortality of beetles exposed to different concentrations of 
 carbon disulfide $CS_2$?


</br>

## <i class="fas fa-table"></i> Type of data {.tabset .tabset-fade}

</br>

### Theory

As mentioned above, in a Binomial GLM the response data are always binary. 
The values of the response variable should have only two outcomes. Examples of 
this type of data would be:

 * Survived/died
 * Present/absent
 * Yes/no
 * On/off

Just as was stated for the linear models, 
**always remember to check that the way your variables are classified in R 
is the same as the format you expect.** 

In this case, there are several options that will work. The response variable 
$Y$ can be stored as a factor, an integer, or as a continuous numeric variable. 
However, it is important that it only has two levels (if it is a factor) or 
contains only 0s and 1s if it is numeric or integer. 

### Worked example 

For this worked example, we will try to find out if/how body weight (kg)
influences the survival probability of Soay sheep. 

```{r sheep-fig, echo=FALSE, eval=TRUE, fig.cap="Illustration of sheep by Emily G. Simmonds", fig.height=150}
include_graphics("./Figures/sheep.png")
```

Biologically, it is expected that smaller sheep would find it harder to survive
harsh winters as they have fewer reserves than larger sheep. 

#### Introduction to the data

This example uses have data on some sheep. The data are from 1986 to 1996 for a
population of Soay sheep from the St. Kilda islands in Scotland, UK.  
They have been studied in a standardised way since 1985. Information on the project can be found [here](http://soaysheep.biology.ed.ac.uk/). These data are
open source and can be found in the appendix of this [paper](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1600-0706.2012.00035.)CITE COULSON 2012.

The data consists of five variables: 

* Year - year of recording
* Age - age of the sheep in years
* Survival - whether the sheep survived until the next year or not
* Weight - weight in kg 
* Population size (PopSize) - number of females counted that year

All measures are for female sheep only. 

You can find the data [here]() if you want to follow along with the example. 
It is a `.csv` file with column headings.

To begin the analysis, let us first have a look at the data after it 
has been imported. We do this by looking at the raw data and making a plot. It
can be easier to see the data if we jitter it because many data points sit on 
top of each other. 

```{r sheep-data-plot, include = TRUE, echo = TRUE, fig.cap = "Scatterplot of weight and against survival of sheep, plot a = raw data, plot b = jittered points"}

plot1 <- ggplot(sheep_data, aes(x = Weight, y = Survival))+
 geom_point(colour = "grey70", size = 4)+
 labs(x = "Weight of sheep (kg)",
      y = "Survival of sheep",
      title = "a")

plot2 <- ggplot(sheep_data, aes(x = Weight, y = Survival))+
 geom_jitter(colour = "grey70", size = 4, height = 0.1)+
 labs(x = "Weight of sheep (kg)",
      y = "Survival of sheep",
      title = "b")

plot1 + plot2 + plot_layout(nrow = 1)
 
```

Since we are interested in 
**whether weight (kg) influences the survival of sheep**, 
we will need two of the five variables in the data: `Weight` and `Survival`. 
We will not need `Year`, `PopSize` and `Age`. 

The response variable then is `Survival`, 
which is a binary variable (it has two outcomes 0/1). 
The explanatory variable is `Weight`, which is continuous numeric. 

</br>

## <i class="fas fa-project-diagram"></i> Model details {.tabset .tabset-fade}

</br>

### Theory

</br>

A Binomial GLM is just like all other statistical models, it aims to represent
mathematically how the data that are being modelled were generated. 
In this case,
it is assumed that the data were generated by a Binomial distribution and 
influenced by a relationship with some explanatory variables. 

The sections below detail the three components of a GLM and describe what form 
they take for the Binomial GLM: 

* The <span style="color:orange">systematic part</span>: $\alpha$ + $\beta$$X_i$
* The <span style="color:purple">random part</span>: the distribution of the error
* The <span style="color:magenta">link function</span>: that links them together

</br>

##### The <span style="color:orange">systematic part</span>

This is the part of the model that is hopefully the most familiar, it is a
linear equation just like those used in linear models. 

This is also known as the linear predictor, $\eta$. 

An example is 
$\eta$ = $\alpha + \beta_1 X_{1} + \beta_2 X_{2}$

where $\alpha$ = an intercept, and the $\beta$ values represent the change in 
$\eta$ with every unit change of $X$. 

</br>

##### The <span style="color:purple">random part</span>

This part of the model represents the spread of the actual data points around
the linear predictor. 
This random part is how we deal with the errors (residuals) of the model
<span style="color:red">$\varepsilon$</span>. 
In linear models, this part was assumed to be Normal. 
In a Binomial GLM, it is assumed to be Binomially distributed (the clue is in 
the name). 

The Binomial distribution is used to represent the number of 
successes ($r$) from a number of independent trials ($N$) when
the probability of success ($p$) is the same for each trial. We
use it for binary data, 0s and 1s.

The Binomial distribution has two parameters $N$ and $p$. Usually,
$N$ is known (it comes from the data), 
so there is only one unknown parameter that must be estimated.
That is $p$. 

#### The <span style="color:magenta">Link function</span>

The link function transforms the <span style="color:orange">systematic part</span> 
of the model onto the scale of data, connecting it to the 
<span style="color:purple">random part</span>. 

This step is necessary because the model is non-linear. If you were to directly
estimate $Y$ (the response data) with a linear predictor as you do in a linear
model, this would give a straight line. The straight line would not be bounded
and could therefore predict values of $Y$ above 1 or below 0, which does not
make any sense. 

Instead, the Binomial GLM links the estimate from the linear predictor to $Y$ 
via a link function. 

The default link function, or canonical function, for a Binomial distribution
is the logit function (hence the alternative name of logistic regression). 
So,
in a Binomial GLM the logit of the linear predictor is linked to $Y$ instead
of the linear predictor itself. 

The logit link takes the following form:

$$
\begin{aligned}
\mu = log(\frac{p}{1-p})
\end{aligned}
$$

Where $\mu$ = the log odds, which is the output of the model.

The inverse of the logit link is:

$$
\begin{aligned}
p = \frac{e^{\mu}}{1+e^{\mu}} 
\end{aligned}
$$

OR

$$
\begin{aligned}
p = \frac{1}{1+e^{-\mu}} 
\end{aligned}
$$

Where $p$ = the probability of success.

Here $\mu$ is on the link scale and $p$ is on the original scale. 

</br>

Other possible link functions for a Binomial GLM are the 
**probit** and **cloglog** functions.

For more information </details></summary>click here</summary>

### The Probit link

One way of thinking of binomial problems is as a threshold:

e.g. imagine you have a dam, and if the water is too high, it will flow over the dam. 

Therefore a model is needed that can capture when the dam overflows (it either does or does not). 
What is observed, and modelled, is whether the water was too high, or not. 
But, hidden underneath this is a variable of water height, which is not observed. 
But it is water height that controls if the dam overflows or not. 
An unobserved variable like this is called a **latent variable**.

This idea can be used in the modeling. 
If the water height was observed, it could be modelled with a simple linear regression 
of water height against rainfall, and assume the residuals are normally distributed. 
But, if only data on whether this value is above a certain threshold, or not is available. 
It turns out that this is the same as a Binomial GLM with a probit link!

Mathematically the model is $Y_i = 1$ if $\mu_i > 0$, where $\mu_i$ is the latent variable. 

The threshold idea can be useful for interpreting models: 
if you think there is some unobserved variable that causes the binary response when it is above a threshold, 
it can be easier to understand the process. 

In practice the estimates from the probit and logit link functions give almost the same results, 
even though they have different interpretations.

The probit uses an inverse normal link function where a higher mean =
higher probability of success.

**Use when you want a threshold model because you have an unobserved variable that causes a binary response.**

### cloglog

Using a cloglog link allows binary data to be linked to count data.

It is useful when the 0s and 1s actually come from counts, where the count is recorded as "0/zero" or "more than zero". 
For example, the presence or absence of a species. 
In this case, the presence or absence is really a result of counting abundance of
a species. 

The cloglog link allows these binary data to be linked to a log(abundance) 
using the equations below.

$$
log(\lambda) = log(-log(1-p))
$$

Where $\lambda$ = the mean abundance and $p$ = probability of 
presence. 

**Use when you want to link the binary data to abundance because they represent counts.**

</details>

</br>

#### Assumptions

There are several assumptions that should be met for the Binomial GLM to be 
valid.

 * There are no outliers

 * Each value of Y is independent

 * The dispersion parameter is constant

 * The correct variance function is used (in a Binomial GLM this is assumed
 to be controlled by the mean)
 
 * The correct distribution is used (Binomial here - it is assumed that the 
 random part of the model does follow a Binomial distribution)

All of these assumptions should be met for the model to work properly and 
they ALWAYS need to be checked. Outliers, dispersion, and variance can be 
checked once the model has been fitted. 

Independence of Y should be ensured during data collection and the correct
distribution is determined by the characteristics of the data.

</br>

#### Writing the model in <i class="fab fa-r-project"></i>

To fit the model in <i class="fab fa-r-project"></i>, we will use the `glm()` function. 

`glm(y ~ x, data, family=binomial(link = logit))`

The `glm()` function takes several arguments:

 * formula in form: `y ~ x` (<span style="color:orange">systematic part</span>)

 * data: your data object.

 * family: specifies the distribution used for the 
 <span style="color:purple">random part</span> of the model and the 
 <span style="color:magenta">link function</span>. 
 In this case, 
 the model uses the binomial family with a logit link. 

The function will fit the GLM using **maximum likelihood estimation**
and gives us the maximum likelihood estimates of 
<span style="color:orange">$\alpha$</span> and
<span style="color:blue">$\beta_is$</span> as an output. 

The formula part `y ~ x`, is the same as in the function `lm()`.
The $y$ is the response variable and $x$ is an explanatory variable. 
To run the GLM in <i class="fab fa-r-project"></i> first it is necessary to
identify which variable is the response and which is/are 
explanatory.

</br>

Once you have an identified response and explanatory variable/variables, you
can then plug these variables into the `glm()` function in the below format 
using the column names in place of `response` and `explanatory` and 
including your data frame name as the data argument. 

</br>

```{r model-example, include=TRUE, echo=TRUE, eval=TRUE}

glm_model_object <- glm(response ~ explanatory, data = your_data,
                     family = binomial(link = "logit"))

```

Running the `glm()` as illustrated above runs the GLM 
and saves the output as a `glm_model_object`.

We can then view our results from the model object by using the function `coef()`. 
This will take the output of the `glm()`, the model object, 
as its argument and extracts the maximum likelihood estimates of 
<span style="color:orange">$\alpha$</span> and 
<span style="color:blue">$\beta$</span>.

```{r coef-example, include = TRUE, echo = TRUE}

coef(glm_model_object)

```

</br>

### Worked example 

</br>

This worked example demonstrates how to fit a GLM in $R$ 
using the `glm()` function for the Soay sheep data example. 

In this example, we are asking:
**Does body weight affect the survival probability of sheep?**

The survival of the sheep (0 or 1) is the response ($Y$) and 
body weight (kg) ($X$) is the explanatory variable. 

We put these variables in the `glm()` function in the below format.

```{r sheep-model, include = TRUE, echo = TRUE}

sheep_model <- glm(Survival ~ Weight, data = sheep_data, 
                   family = binomial(link = "logit"))

```

We have run the model and have assigned it to an object name. A logit link was
used as these data do not come from a latent continuous variable or from counts, 
so we can use the default/canonical link function here. 

Let us take a look at the maximum likelihood estimates of our model parameters (<span style="color:orange">$\alpha$</span>, 
<span style="color:blue">$\beta$</span>) 
using the `coef()`.

```{r coef-sheep-model, include = TRUE, echo = TRUE}

coef(sheep_model)

```


## <i class="fas fa-laptop"></i> Parameters {.tabset .tabset-fade}

</br>

### Theory

We introduced the model parameters of a Binomial GLM in the model theory
section above: 
<span style="color:orange">$\alpha$</span> = 
the intercept; <span style="color:blue">$\beta_i$</span> = 
the slope of the model line (steepness/gradient).

**But what do these parameters really mean?**

To fully understand the parameters and what they mean, we have to note that 
there is a non-linear relationship between the explanatory and response 
variable on the scale of the observed data. 

To interpret the parameters, 
we have to recall the linear predictor and the link function.

The linear predictor is $\eta = \color{orange}\alpha + \color{blue}{\beta_i}X_i$. 
The $\eta$ in this case is the log odds of the probability of success ($p$).  
$p$ is what we are trying to estimate. 
That is:

$$
log\bigg(\frac{p}{1-p}\bigg) = \eta = \color{orange}\alpha + \color{blue}{\beta}X_i.
$$

You might recognise the first part of this formula from the introduction of the
logit link function above, it is the logit link. Note that on the scale of the
link, the model line will be straight. However, on the original scale will be 
curved. 

To relate the linear predictor back to the original scale of the data and $p$. 
It is necessary to take the inverse of the link function like using 
the below equation:

$$
p = \frac{1}{1 + e^{- \eta}} = \frac{1}{1 + e^{-(\color{orange}\alpha + \color{blue}{\beta}X_i)}}.
$$

Therefore, it is possible to interpret the parameters on the original scale 
or on the link scale. We will discuss both options here. 

</br>

#### Parameters on the logit scale

On the logit scale, the response will be the log odds of $Y$. 

Odds are often used in betting.
Odds of 10:1 mean for every 1NOK you bet, you win 10NOK.
The betting companies assume that for every 1 success
there will be 10 failures.

To get to the log odds there are a few steps.

1. We need the probability of success: $\frac{1}{(1+10)}$ = $0.09$

2. Next we use that probability to find the odds:
$\frac{0.09}{(1-0.09)} = 0.0989$
 
3. To get to the log odds, we take the log of 2:
$log(\frac{0.09}{(1-0.09)}) = -2.31$

Step 3 should look familiar. It is the same as the link 
function in the Binomial GLM. 

**<span style="color:orange">$\alpha$, the intercept</span>**

This is the parameter that gives the value of the log odds of $Y$ when $X = 0$.

**<span style="color:blue">$\beta$, the slope</span>**

This gives the amount of change in the log odds of $Y$ as the value of $X$ 
changes by one unit. 

```{r log-odds-plot, include = TRUE, echo = FALSE, fig.cap = "Example of a Binomial GLM model line on the logit scale. Intercept in orange and slope in blue"}

example_data <- mutate(example_data,
                       logity = predict.glm(glm_model_object, 
                                            newdata = example_data,
                                            type="link"))

ggplot(example_data, aes(x = explanatory, y = logity))+
 labs(x = "x",
      y = "log odds of y",
      title = "Example of results of GLM on logit scale")+
  geom_line(aes(x = explanatory, y = logity),
            colour = "blue")+
  geom_point(aes(x = 0, y = coef(glm_model_object)[1],
                 colour = "orange"), size = 4)+
  theme(legend.position = "none")
 
```


#### Parameters on the original scale

On the original scale, the response will be $Y$, which is a probability of 
success (e.g. survival, presence etc). 

**<span style="color:orange">$\alpha$, the intercept</span>**

To find the expected value of $Y$ when $X = 0$. You need to convert the 
estimate of $\alpha$ from the logit scale back to the original scale using the
equation for the inverse of the logit link: e.g.

$$
p = \frac{1}{1+e^{-(\alpha)}}
$$

This can be done in R.

```{r intercept-convert, include = TRUE, echo = TRUE}

# estimate of the intercept on logit scale (log odds of y)

coef(glm_model_object)[1]

# convert to original scale (a probability of success)

1/(1 + exp(-(coef(glm_model_object)[1])))
 
```

**<span style="color:blue">$\beta$, the slope</span>**

On the original scale the slope parameter has a somewhat different meaning. On
the logit scale, the slope captured a linear relationship. 
However, on the 
original scale, the relationship between $X$ and $Y$ is not linear. 
As a result,
the gradient of the model line will be different for different $X$ values 
(see \@ref(fig:original-scale-example)). 

```{r original-scale-example, include = TRUE, echo = FALSE, fig.cap = "Example of a Binomial GLM model line on the scale of the original data. Intercept in orange and slope in blue", fig.height=150}

example_data <- mutate(example_data, 
                       predictions = predict(glm_model_object,
                                             example_data,
                                             type = "response"))

ggplot(example_data, aes(x = explanatory, y = response))+
 geom_point(aes(x = explanatory, y = response),
            colour = "grey70", size = 3)+
 labs(x = "x",
      y = "y",
      title = "Example of results of GLM on original")+
  geom_line(aes(x = explanatory, y = predictions),
            colour = "blue")+
  geom_point(aes(x = 0, y = 1/(1+exp(-(coef(glm_model_object)[1]))),
                 colour = "orange"), size = 4)+
  theme(legend.position = "none")
 
```

The estimate of the $\beta$ coefficient on the logit scale, will tell you about
the strength and direction of the relationship between $X$ and $Y$. 
This is the
case even though this relationship will not be linear on the original scale.

To see the change in slope of the model line on the original scale you can
predict values of $Y$ on the original scale and either plot the relationship 
(like above in \@ref(fig:original-scale-example)) or
simply look at the change between two $X$ values. 

e.g.

```{r slope-convert, include = TRUE, echo = TRUE}

# predict log odds of Y when X = 2 or X = 3
# use a linear equation for this

odds_y_x2 <- coef(glm_model_object)[1] + (coef(glm_model_object)[2]*2)
odds_y_x3 <- coef(glm_model_object)[1] + (coef(glm_model_object)[2]*3)

# estimate of probability of success when X = 2
# take inverse of the logit link

prob_X2 <- 1/(1 + exp(-odds_y_x2))

# estimate of probability of success when X = 3

prob_X3 <- 1/(1 + exp(-odds_y_x3))

# slope from X = 2 to X = 3 on original scale

prob_X3 - prob_X2
 
```

If you were to repeat the example above for $X$ = 10 and $X$ = 11, you would not
get the same slope on the original scale. 
Even though it is the same on the logit scale. 

</br>

### Worked example 

In the previous section, we fit a Binomial GLM using the `glm()` function and 
looked at the estimates of some parameters using the `coef()` function. 
In this section, we will use the model theory to interpret what those parameters
mean. 

#### <span style="color:orange">The intercept</span>

For our sheep data model, the estimate of the intercept is:

```{r coef-sheep-model2, include = TRUE, echo = TRUE}

coef(sheep_model)[1]

```

This is the estimate of the log odds of survival for a sheep which has a body weight of 0 kg. 
The intercept estimate is a log odds of `r coef(sheep_model)[1]` of survival 
for a sheep that weighs 0 kg. 

To get the probability of survival for a sheep with a body
weight of 0 kg we need to convert these estimates to the original scale using
the code below:

```{r back-transform-sheep, include = TRUE, echo = TRUE}

1/(1+exp(-coef(sheep_model)[1]))

```

In other words, 
the survival probability of a sheep is `r 1/(1+exp(-coef(sheep_model)[1]))` 
when their weight is 0 kg. 

In this example, the intercept is not very interesting, 
as it does not make a lot of biological sense to know the expected survival 
probability of sheep when their weight is 0 kg. 
A sheep born at anytime will weigh more than 0 kg. 

#### <span style="color:blue">The slope</span>

The estimate of the slope of the relationship between body weight and 
survival probability is interesting to us since it can tell us the direction
and the strength of the effect of body weight on survival of the sheep. 


```{r coef-sheep-model3, include = TRUE, echo = TRUE}

coef(sheep_model)[2]

```

In this case, our model estimates that the log odds of the survival of the sheep
increases by `r coef(sheep_model)[2]` when the weight of the sheep increases by
1 kg. This shows a positive relationship between body weight and survival 
probability. 

To see what this effect looks like on the original scale, it is easiest to
plot the results using predictions on the response scale (original scale). This
is what we do below. 

#### Plotting the Results 

As well as looking at the maximum likelihood estimates of the parameters from 
the Binomial GLM, we can also plot the results. This is especially useful for
interpreting the results on the original data scale. 

To do this, we use the `ggplot()` with `geom_line()` and the `predict()`
function.

To make the first plot, we will only need to use two arguments:

 * `object` = your model object
 * `type` = "link", which means predict on the link scale or
 "response", which means predict on the response scale. 
 
We will make predictions on both the link and original scale so that we 
can compare the results. 

```{r sheep-predictions1, include = TRUE, echo = TRUE}

sheep_data <- mutate(sheep_data, 
                     predictions_link = predict(sheep_model, 
                                            type = "link"),
                     predictions_original = predict(sheep_model, 
                                            type = "response"))

```

Once we have created predictions of $Y$ from the model object, 
we can plot these using `geom_line()` as in the code below.

```{r fig-sheep-preditions1, include = TRUE, echo = TRUE, fig.cap = "a) Plot of fitted GLM line on the logit scale b) Plot of jittered raw data and fitted GLM line for the soay sheep data on the original scale"}

link <- ggplot(sheep_data, aes(x=Weight, y=predictions_link))+
  geom_line(aes(y=predictions_link), colour = "blue")+
  labs(x = "Weight (kg)", y = "Log Odds of Survival")

original <- ggplot(sheep_data, aes(x=Weight, y=Survival))+
  geom_jitter(colour = "grey70", size = 2,
              height = 0.05)+
  geom_line(aes(y=predictions_original), colour = "blue")+
  labs(x = "Weight (kg)", y ="Survival Probability")

link + original + plot_layout(nrow = 1)

```

</br>

In the next section, we will look at how to add uncertainty to these plots and 
our interpretation. 

</br>


## <i class="fas fa-arrows-alt-h"></i> Quantify uncertainty {.tabset .tabset-fade}

</br>

### Theory

As was discussed for linear models, 
statistics does not give a single correct answer. 
When we estimate the parameters in our statistical model, 
there will be many plausible parameters that could have produced our 
observed data. In these cases, some of the parameters will be more likely
than the others to have generated the observed data.

The Binomial GLM is no exception. 
We will have to consider and present the uncertainty in the parameters we 
estimate.

The `glm()`function uses maximum likelihood estimation for the parameter 
estimation. Therefore, our consideration of uncertainty for these models are 
discussed [here](link to ML page). 
We will therefore quantify uncertainty using *standard errors*, 
*confidence intervals* and *prediction intervals* 
which should be familiar to you but head to the [uncertainty]() 
pages if you need a recap.

Just as with linear models there are two different types of uncertainty 
we will look 
at *uncertainty in the model line of parameters $\alpha$ and $\beta$* and 
*uncertainty in a prediction of $Y$*.

</br>

##### Uncertainty in the estimates of $\alpha$ and $\beta$

To find the standard errors for the estimates of $\alpha$ and $\beta$ 
we can use `summary()` function. 
This takes the model object from the `glm()` as its argument. 
It outputs a big table with lots of information. 
The first line gives the formula used for the model object. 
The second line shows the summary of the residuals of the model and the 
standard errors are shown in the second column of the third part, 
`Coefficients:`.

```{r summary_glm_example}
summary(glm_model_object)
```
</br>

By taking the `summary()` of the example model, we can see the standard error of 
the intercept ($\alpha$) is `r summary(glm_model_object)$coefficients[1,2]`, 
and the standard error for the slope 
($\beta$) is `r summary(glm_model_object)$coefficients[2,2]`.

We also observe that the standard error is smaller than the estimated effects. 
Actually, to indicate a clear direction of the effect, the standard error should
be less than half the size of the estimated coefficient. This would ensure that
95% confidence intervals for the effect (see below) do not span 0. 

</br>

##### Confidence intervals

For interpretation of the uncertainty, it can be easier to use the standard 
error to calculate confidence intervals. 
Confidence intervals indicate the range of plausible values for a parameter. 
**They represent an interval, that if you were to collect a sample and run the
analysis, then
repeat that many many times AND each time
draw a confidence interval, on average 95% of the time, 
the true population value of the parameter would be found in 
within the confidence interval.** 

</br>

To calculate a confidence interval from a standard error you need to use the 
formulas (this is exactly the same as for linear models):

$$
\begin{aligned}
UpperCI = estimate + (1.96 SE) \\
LowerCI = estimate - (1.96 SE) \\
\end{aligned}
$$
Remember that the estimates of the coefficients are for the effect of 
$X$ on the log odds of $Y$ (i.e. on the link scale). Therefore any measures of 
uncertainty are also on the link scale. 
After estimating the upper and lower confidence intervals of these parameters, 
it might be useful to convert them to the original scale to understand their
effect. This is especially true for the intercept ($\alpha$). 

</br>

Using the `confint()` function it is possible to display the 95% confidence 
intervals for the intercept and
slope.

```{r confint_glm_example}

confint(glm_model_object)

```


</br>

##### Plotting uncertainty in $\alpha$ and $\beta$

The above section has shown how you can quantify uncertainty in the line 
parameters of a Binomial GLM as numbers. 
But often in science, it is clearer to show these things visually.

We can add the confidence intervals to \@ref(fig:log-odds-plot) 
showing our GLM fitted line. This can be done either on the link scale or the
original scale. 

To do this, we need to generate new predictions with the `predict()` function. 
We also need to calculate the upper and lower confidence interval bounds using
the standard error. 

This is a little different to using `predict()` for a linear model object. For
GLMs there are some different arguments. 
The key arguments are:

* `type`. This argument can take values "link" (gives predictions on the link
scale), "response" (gives predictions on the original response scale) or "terms"
(gives a matrix of the fitted values of each term in the model formula on the
link scale).
* `se.fit`. This argument can be `TRUE` or `FALSE` and indicates whether the 
standard error for the prediction should be given in addition to the mean 
prediction. The standard error will be on the scale of the mean prediction i.e.
either link scale or original dependent on which `type` was specified. 

</br>

The `interval` argument that was used for linear model objects to specify a
prediction or a confidence interval does not work for predictions on a GLM
object. Therefore, the standard errors presented are for a 95% confidence 
interval. 

```{r preditions_example, echo = TRUE}

# First create the new data to predict for 
# This example generates 100 values of the explanatory variable from min to max

newdata <- data.frame(explanatory = seq(min(example_data$explanatory),
                                        max(example_data$explanatory),
                                        length.out = 100))

# Then add predictions to the newdata for the link and original scale
# REMEMBER the confidence intervals too, you can access these by taking the 
# $se.fit of the results of `predict`, $fit is the mean prediction

predictions <- newdata %>% mutate(predictions_link = predict(glm_model_object, 
                                                         newdata = newdata, 
                                                         type="link", 
                                                         se.fit=TRUE)$fit,
                              SE_link = predict(glm_model_object, 
                                                         newdata = newdata, 
                                                         type="link", 
                                                         se.fit=TRUE)$se.fit,
                              predictions_original = predict(glm_model_object, 
                                                         newdata = newdata, 
                                                         type="response", se.fit=TRUE)$fit,
                              SE_original = predict(glm_model_object, 
                                                         newdata = newdata, 
                                                         type="response",
                                                         se.fit=TRUE)$se.fit)


predictions

```
</br>

You should notice that all predictions on the original scale around bounded 
between 0 and 1, but this is not the case on the link scale. 

</br>

Then you can plot these. Remember these are confidence intervals not prediction
intervals. The `predict` function will not give prediction intervals for a GLM. 
In fact, it is not always an easy or sensible thing to create in this context,
especially for a Binomial GLM. When the mean prediction is close to 0 or 1, the
prediction interval can easily fill the whole possible value space (i.e. span
from 0 right to 1), which is not very logical and is hard to interpret. 

To get the upper and lower bounds of the confidence interval from
a GLM using `predict()` you need to calculate the bounds yourself. 
You need to add or
subtract 1.96 times the standard error from the mean prediction. 

```{r plot_ci_example, fig.caption = "Examples of plots of fitted GLM line and 95% confidence intervals (grey shaded area) on the link scale (A) and on the original response scale (B)"}

predictions <- predictions %>% mutate(upper_ci_link = predictions_link + (2*SE_link),
                                      lower_ci_link = predictions_link - (2*SE_link),
                                      upper_ci_original = predictions_original + (2*SE_original),
                                      lower_ci_original = predictions_original - (2*SE_original))

# plot predictions on the link scale 

example_predictions_link <- ggplot(data = predictions, 
                                 aes(x = explanatory, 
                                     y = predictions_link))+
  geom_line(color="blue")+
  geom_ribbon(aes(ymin = lower_ci_link, 
                  ymax = upper_ci_link), 
              alpha = 0.5)+
  labs(title = "A. Predictions on link scale",
       y = "Log odds of response",
       x = "Explanatory")

# For this plot, the base can be the original data
example_predictions_original <- ggplot() +
  geom_jitter(data = example_data, 
              aes(x = explanatory, y = response), 
              colour = "grey", size = 2, height = 0.01) +
  geom_ribbon(data = predictions,
              aes(x = explanatory,
                  ymin = lower_ci_original, 
                  ymax = upper_ci_original), 
              alpha = 0.5) +
  geom_line(data = predictions, 
            aes(x = explanatory, y = predictions_original), 
            colour = "blue") +
  labs(title = "B. Predictions on original scale",
       y = "Probability",
       x = "Explanatory")

example_predictions_link + example_predictions_original

```


### Worked example 

At the end of the last section, we created of our sheep model and looked at the 
estimated model line. 
Now we will add uncertainty to that plot.
 
First, we should look at the confidence intervals of our parameter estimates.
 
```{r}

round(confint(sheep_model),2)

```

The confidence interval has been rounded to two decimal places to make it 
easier to read. 

To add these intervals to the plot, 
we need to make new predictions including the upper and lower bounds of the 
prediction interval. We will do this on the link scale and the original 
response scale. 


```{r sheep_predictions}

# first make some new data to predict for
newdata <- data.frame(Weight = seq(min(sheep_data$Weight),
               max(sheep_data$Weight),
               length.out = 100))

sheep_predictions <- newdata %>% mutate(predictions_link = predict(sheep_model,
                                                    newdata,
                                                    type="link", 
                                                    se.fit = TRUE)$fit,
                            # the next lines add the lower and upper ci bounds
                            lower_ci_link = predict(sheep_model,
                                                    newdata,
                                                    type="link", 
                                                    se.fit = TRUE)$fit - 
                                                    (1.96*predict(sheep_model,
                                                    newdata,
                                                    type="link", 
                                                    se.fit = TRUE)$se.fit),
                            upper_ci_link = predict(sheep_model, 
                                                    newdata,
                                                    type="link", 
                                                    se.fit = TRUE)$fit + 
                                                    (1.96*predict(sheep_model,
                                                    newdata,
                                                    type="link", 
                                                    se.fit = TRUE)$se.fit),
                            # then predict on the original scale
                            predictions_original = predict(sheep_model,
                                                    newdata,
                                                    type="response", 
                                                    se.fit = TRUE)$fit,
                            # and add the confidence interval bounds
                            lower_ci_original = predict(sheep_model,
                                                    newdata, 
                                                    type="response", 
                                                    se.fit = TRUE)$fit - 
                                                    (1.96*predict(sheep_model,
                                                    newdata,
                                                    type="response", 
                                                    se.fit = TRUE)$se.fit),
                            upper_ci_original = predict(sheep_model,
                                                    newdata, 
                                                    type="response", 
                                                    se.fit = TRUE)$fit + 
                                                    (1.96*predict(sheep_model,
                                                    newdata,
                                                    type="response", 
                                                    se.fit = TRUE)$se.fit))

```
</br>

Once we have created predictions of $Y$ from the model object, 
we can plot them using `geom_line()` and `geom_ribbon()` as in the code below.

```{r sheep_predictions_plot, fig.caption = "A. plot of the fitted GLM line on the link scale including 95% confidence interval (grey), B. plot of the fitted GLM line on the original scale including 95% confidence interval (grey)"}

# Plot of Confidence Intervals on the link scale

sheep_predictions_link <- ggplot(data = sheep_predictions, 
                                 aes(x = Weight, 
                                     y = predictions_link))+
  geom_line(color="blue")+
  geom_ribbon(aes(ymin = lower_ci_link, 
                  ymax = upper_ci_link), 
              alpha = 0.5)+
  labs(title = "A. Predictions on link scale",
       y = "Log odds of survival",
       x = "Body weight (kg)")

# For this plot, the base can be the original data
sheep_predictions_original <- ggplot() +
  geom_jitter(data = sheep_data, 
              aes(x = Weight, y = Survival), 
              colour = "grey", size = 2, height = 0.01) +
  geom_ribbon(data = sheep_predictions,
              aes(x = Weight,
                  ymin = lower_ci_original, 
                  ymax = upper_ci_original), 
              alpha = 0.5) +
  geom_line(data = sheep_predictions, 
            aes(x = Weight, y = predictions_original), 
            colour = "blue") +
  labs(title = "B. Predictions on original scale",
       y = "Probability of survival",
       x = "Body weight (kg)")


sheep_predictions_link + sheep_predictions_original

```

It can be observed from the figure above that the confidence intervals for the 
predictions around $1$ are very narrower. 
This means that as we get closer to the true classification (0 or 1), 
we are able to estimate them with much certainty.

</br>

## <i class="fas fa-tasks"></i> Model checking {.tabset .tabset-fade}

</br>

### Theory

Till this point, we have the model, parameter estimates and uncertainty 
quantification for the binomial GLM.
**We then discuss how to know if the model is good.**

Under the linear models, 
we used **Residuals vs fitted plots** for equal variance and linearity, 
**the normal Q-Q plots** for normality of residuals and the **Cooks distance**
for the outliers. 
These are easier to interpret since we know what we are looking for. 
However, this is not the case for GLMs since they have non-normal variance. 

To handle this the non-constant variance, 
we aim at producing plots that are roughly normal. 
There are two ways of doing: **Pearson** and **Deviance** residuals. 
These are not perfect in any way. 
These measures scale the residuals by variance in some way. 
Once we have scaled the residuals in this way to account for non-equal variance, 
they should be approximately normal.

**Pearson residuals**

Thsís is defined as 
$$
Res_p = \frac{x - \mu_x}{\sigma_x}.
$$

where $\mu_x$ is the mean of the residuals and $\sigma_x$ 
for the variance of the residuals. 

In $R$, the Pearson residuals are obtained by:
```{r, eval = FALSE}
residualsP <- resid(ExampleModel, type = "pearson")

```

**Deviance Residuals**

This is defined:
$$
Res_D = sgn(y_i - E(y_i))\sqrt{D_i}
$$
where $sgn(x)=1$ when $x>0$ and $-1$ when $x<0$. 

The deviance residuals is the default in the `glm()`. 

In $R$, the Deviance residuals are obtained by:
```{r, eval = FALSE}
#residualsD <- resid(ExampleModel, type = "deviance")
```

</br>



</br>


#### 1. Residuals vs fitted plot

This plot tests the assumptions of 
**The relationship between X and Y is linear**, 
**The residuals have a mean of 0**, and 
**The variance of the residuals is equal for all fitted values 
(homoscedasticity)**. Remember that we use the scaled residuals 
(Pearson or Deviance) instead of the original residuals.


</br>
```{r, eval = FALSE}
#fitted <- fitted(ExampleModel) #extract fittedvalues
#
#par(mfrow=c(1,2))
#plot(fitted, residualsP, main = "Pearson")
#abline(h=0)
#
#plot(fitted, residualsD, main = "Deviance")
#abline(h=0)
```
*Figure 8: Residuals vs fitted plot for the Example model.*

</br>

The Pearson and Deviance residuals showed similar structure. 
Around the point zero, the residuals decreased with increase in the 
fitted values. The plot shows a linear trend, but we do expect the points to be 
randomly scattered around the point $0$. 
This suggests that our model has an issues with the linearity and equal 
variance. 
To solve this, we can try and add another covariate to explain some of the 
variation and as such get the model better.


</br>


#### Normal QQ plot

This plot tests the assumption of **the residuals are normally distributed**. 

</br>
 If our assumptions
are met, we expect **The points lie along the line.** 
The line represents a perfect theoretical
normal distribution, the points come from our model.

```{r, eval = FALSE}
#qqnorm(residualsD)
#qqline(residualsD)
```
*Figure 9: normal QQ plot for the example model.*

</br>
In our model, 
it was observed that the points do not lie along the straight line. 
Although we have had a tweak the residuals, 
they are still not normally distributed. 
This supports the assertion that other covariates should be added to model to 
improve its prediction.

We also observe that there are some outliers in our data, 
which is quite normal to observe from a QQ-plot. 
We find that some points lie extremely from the others. 
These points would be known however from the next model checking. 

</br>

#### Cook's Distance

This plot tests the assumption of **no outliers**. 
It lets you identify influential points. 
We look for significant values of the Cook's distance, about $0.02$.

</br>

```{r, eval = FALSE}
#plot(ExampleModel, which = 4)
```
*Figure 10: Cook's Distance plot for the Example model.*

</br>

<details><summary>Check your answer</summary>

The point 132, 124 and 112 appears quite different to all others, 
they have more influence.

To decide if points 132, 124 and 112, 
we would need to go back to the data and check
for typos, mistakes, or anything that could be wrong with these points. 

```{r, eval = FALSE}
#example_data[c(112,124,132),]
```

The weights at these points are `23.2`, `22.8` and `22.6`. 
However, a quick histogram of the weights (figure 11) do not suggest that 
these points are very far from the others. 
Hence we do not not discard these values. 

```{r, eval = FALSE}
#hist(example_data$Weight, main = "")
```
*Figure 11: Histogram of the weights of the sparrows.*

</details>

</br>


</br>

### Worked example 

</br>

Using the theory covered in the previous section, 
we can now check our sheepModel 
to ensure that it meets the assumptions of a Binomial GLM.

</br>

We will use one plot at a time to test specific assumptions.

</br>

#### Residuals vs fitted

```{r, echo=T, message=F}
#resP <- residuals(sheepModel, type="pearson")
#resD <- residuals(sheepModel, type="deviance")
#
#fitted <- fitted(sheepModel) #extract fittedvalues
#
#par(mfrow=c(1,2))
#plot(fitted, resP, main = "Pearson")
#abline(h=0)
#
#plot(fitted, resD, main = "Deviance")
#abline(h=0)

```
*Figure 12: Residuals vs fitted plot for the sheep model.*

 </br> 
 
Figure 12 looks quite unusual. 
There are a few assumptions we are checking with this plot:


* **Do the residuals have a mean of 0?** 
The mean of the residual deviance will be approximately $0$.
* **Is the variance of the residuals is equal for all fitted values 
(homoscedasticity)?** It is not the nice cloud of random points that we expect.
**But is it a problem?** To answer this, we need to look at bit closer. 
We observe that the points are not spread randomly around the mean of $0$. 
Moreover, we can also see a linear trend of the upper and lover residuals 
(linearly decaying with increase in fitted values).

#### Normal QQ

```{r, echo=T, message=F}
#plot(sheepModel, which = 2)
```
*Figure 13: Normal QQ plot for dive depth model.*

</br> 

The assumption we are checking with this plot is: 
**are the residuals are normally distributed?**

As expected, there is not a perfect match between the theoretical normal 
distribution and the distribution of the residuals. 
There is some deviation at the lower tail of the distribution. At upper half, 
this seems ok. At lower values, points 47, 107 and 392
deviate quite a lot. These points also stood out in Figure 6. 
We will need to look into them more in Figure 9. 

#### Cook's distance

```{r, echo=T, message=F}
#plot(sheepModel, which = 4)
```
*Figure 14: Cook's distance plot for the dive depth model.*

</br> 

The assumption we are checking with this plot is: **Are there any outliers?**

Figure 9 shows that some the Cook's distances of this model are very high 
(max = 0.2). The values at $46$, $107$ and $392$ have been flagged. 
Hence there is the need to investigate if these points have any 
significant influence on the model. 
We do this by extracting the data points at these places.

</br>

We can find points $46$, $107$ and $392$ by looking at the 46th, 
107th and 392th row of our data frame. 
It can be observed that the sheep with weight $23$ survived but those with 
weight $30.6$ and $32.8$ did not survive. 
From the histogram below (figure 15), 
these values does not deviate from the others. 
This possibly means that weight may be enough to explain the variation of the
survival of the sheep. 
**Therefore, we would not consider this an outlier and would not remove 
from the data.** However, we can focus our attention on getting more covariates 
that can help us explain more the variance and improve the model fit. 

```{r, echo=T, message=F}
#sheep_data[c(47,107,392),]
```

```{r}
#hist(sheep_data$Weight, main="")
```
*Figure 15: Histogram of the weight of the sheep.*

</br>

#### Summary

Overall, it seems that most of the model assumptions are not met well. 
This suggests that `weight` may not be enough to explain the total variation 
in the model. It would be better to fit the model by adding more covariates. 
Therefore, we have to condiser another model.

</br>

In the next section we will interpret our results.

</br> 

</br>


## <i class="far fa-lightbulb"></i> Draw conclusions {.tabset .tabset-fade}

### Theory

In the previous sections you learned how to run a Binomial Generalised linear 
models, what the parameters of the model mean, 
how to quantify uncertainty in the parameters, and 
how to check the assumptions of the model. 
Now, we can bring everything together to draw some conclusions. 

</br>

There are several components required in drawing a conclusion:

 * statement of the maximum likelihood estimate of the parameters of 
interest (including strength and direction).
 * statement of the uncertainty in the estimate
 * link the results to biology and the question asked
 * Discussion of next directions

</br>

#### Example conclusion for example data

The question we asked in this example was 
**Does weight influence the survival of sparrows?**

To answer this, we need an estimate of the strength of the relationship between 
survival and weight. We find this from the slope of our model. 
The maximum likelihood estimate of the slope is 0.3171. 
This corresponds to a postive relationship between weight and survival 
of sparrows where log odds of survival goes up by 0.3171 times for every 
1kg the sparrow puts on. 

</br> 

However, there is some uncertainty around this estimate. 
The 95% confidence intervals of the relationship are 0.0760 to 0.5785. 
The confidence intervals show that the estimate of the intercept has high 
uncertainty relative to the estimate size. 
The confidence interval does not span 0. 
In other words, 0 does not sit between the upper and lower interval because 
they are same signs (both are positive). 
This means that 0 cannot be included in the plausible range of parameter values 
for the relationship (a slope of 0 = no relationship). 
**As a result, we can conclude that there seems to be a significant effect of 
weight on the survival of sparrows at the population level.**

</br> 

Despite having a significant result, our model did not meet the assumptions of
a Binomial GLM reasonably well. However, the sample size was a bit higher (>30),
so having more than can be excused. 
However, collecting more covariates to explain away some of the model variation
would rather improve the model fit.  

</br>

Overall, it seems that weight does have much effect on the survival of sparrows 
based on the data we analysed. 
The postive effect we estimated is strong and when we consider uncertainty, 
the direction of the effect is also clear. 
However, the model failed to meet all the assumptions of the Binomial GLM. 
All of this suggests that there are other drivers that are influencing the 
survival of sparrows that we did not measure here.


</br>

For next steps, 
we might want to collect new data looking at other explanatory variables that 
we think might be more important.
</br>


### Worked example 

This is the final section of our analysis of the data on survival of sheep. 
We will now bring together all of the results we have obtained and draw a 
conclusion following the same format as the sparrow example.

A reminder, we were asking: **Does weight influence survival of sheep?**

</br>

The maximum likelihood estimate of the relationship between weight and survival 
probability is 0.1817. In other words, for every 1 kg increase in body weight,
the log odds of the survival of the sheep increases by 0.1817 times. 

</br>

When we look at the uncertainty in this estimate, we see the 95% confidence 
interval is 0.15 to 0.21. The confidence interval limits are same signs, 
meaning that 0 is not included as a plausible value for the strength of 
the relationship. Therefore, we can conclude that weight has any impact 
on survival of sheep.  

</br>

In model checking, all the assumptions of the Binomial GLMs were not met. 
With the significant relationship of the weight and the survival of the sheep, 
it suggests that the model's total variation cannot be explained only by the 
covariate `weight`. 
Attention should then be focused on collecting more relevant covariates to 
explain some of the variance, 
as we do not have problem with our sample size at the moment. 

</br>

## {.toc-ignore}

### What's next? {.facta}

* **Poisson Generalized linear models** for analyses when your response 
variable is not normally distributed.

### Further reading {.facta .toc-ignore}

### Contributors {- .contributors .toc-ignore}

```{r contributors, eval=FALSE}
* Kwaku Peprah Adjei
* Emily G. Simmonds
* Bob O'Hara
```



