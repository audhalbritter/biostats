
``` {r setup, include=FALSE}
library(knitr)
```

# Data workflow

Here, we will show **best practice data workflows** that are reproducible and transparent.
We cover the whole data life cycle from planning data collection, the data collection itself, data curation, documentation, reporting and sharing.
For example, we recommend to use code-based workflow and to deposite raw, and clean data and the code in open repositories for full transparency (@fig-workflow).


```{r}
#| label: fig-workflow
#| echo: false
#| out-width: "90%"
#| fig-cap: "Reproducible data workflow."

include_graphics("./pics/rep_workflow_FunCaB.jpg")
```


## Digitizing data

The first step after data collection is to digitize the data, unless the data was collected digitally.
When digitizing the data, the digital version should reflect the paper version, to avoid having to flip to different sections of the spreadsheet.
Do not change the format, just to have a long table.
This can easily be changed later.


## Proof reading

After digitizing your data, the next step is to proof read.
Proof reading basically means that you make the digital copy of your data reflect the paper copy.
At this stage, the digital spreadsheet can still be edited by hand, if you are correcting for typos and similar issues.

Note that if you discover consistent error (e.g. entering the wrong date for multiple days), it is safer to make these changes using code.

After proof reading we recommend to save your data as raw data, a non-manipulated version of your data.
Indicate in file name, that this is the raw version of the data.
This will later make the process of data cleaning fully transparent.
For example, if you want to share your data later, anybody can see what was done to the data, and if somebody ever wants to clean your data differently for another purpose, this is still possible.


## Data cleaning

Data cleaning is the process of detecting and correcting or removing, incomplete, incorrect, irrelevant, duplicated or improperly formatted data from a dataset.
Errors and problems in the data can be a problem or limit the downstream data analysis and affect the results.
Therefore, data cleaning can solve some of the problems and improve the data, analysis and their outcome.

The data cleaning should be done fully code-based, meaning that from now on, there should be no more changing things in the data by hand.
Make sure your code is openly available (e.g. on GitHub) to make the data cleaning workflow transparent and reproducible.
For more details see section on data cleaning (@sec-data-cleaning).

After data cleaning, we recommend to save a clean version of your data and indicate again in the file name that this is the clean version.


## Storage

**Save** your data as plain text formats, with comma or tab deliminator.
We recommend csv files, which is a nonproprietary format.
This means that accessing the file does not require any special software and can be opened in any spreadsheet program.


## Backup

**Back up** your data in an data repository that is not connected to your computer to avoid data loss.
There are many options for this and they often have private or closed repositories, which means that you do not automatically need to share the data.

Take pictures of paper versions of your data and store them in a save place.
Pictures can be useful for proof reading.


## Data dictionary and documentation

A dataset on its own is often useless, because it can be difficult to understand what data each column contains.
Therefore, it is best practise to create a **data dictionary** that goes with a dataset (@fig-data-dic).
A data dictionary is a separate file or table that describes the data and explains what each variable means.

This is useful if you want to share your data with collaborators, if somebody else is doing the data analysis for you, or even for your future self in a few years time.
You might not remember exactly what each column means, the units, and how it was measured.

A data dictionary should contain:

- Variable names
- Explanation of the variable
- Unit
- Range/expected min/max

```{r}
#| label: fig-data-dic
#| echo: false
#| out-width: "90%"
#| fig-cap: "Example of a data dictionary"

include_graphics("./pics/data-dic.jpg")
```


Another important part of a dataset is the **documentation** or **metadata** of how the data was collected.
This can be a protocol, a method section in a paper or even a data paper.
What option you choose is up to you.
It is however important that when sharing the data, the user has access to this information.


## Version control

will be added soon.

