[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Enough targets to Write a Thesis",
    "section": "",
    "text": "Before you start\n\n\n\nFor this tutorial you will need:\n\n\nRStudio (version 2022.2 or newer)\nR\nquarto\nquarto R package (install.packages(\"quarto\"))\ntargets R package (install.packages(\"targets\"))\ntarchetypes R package (install.packages(\"tarchetypes\"))\n\nIf quarto is new to you, have a look at the Reproducible documents tutorial (REF to Quarto book) before proceeding here.\n\n\n\n1 Introduction\nWriting a thesis requires importing data (1), writing code to clean, transform (2), analyse data (3), and making figures (5) and writing (6; Figure 1.1). This is done by writing several R scripts and running one script after another producing results and figures. All the time, the code is updated, to add new data, transforming the data, changing analysis or adding a figure. Often many iterations of running the same scripts are needed and it is difficult to keep track of which scripts need rerunning. In addition, complex and computational heavy data analysis can take a lot of time especially when it requires rerunning the analysis.\nThis workflow is very inefficient, non-reproducible and error prone. There is a better way and this tutorial will show you how.\n\n\n\n\n\n\n\nFigure 1.1: Non-reproducible data workflow.\n\n\n\n\nIn this chapter we will first explain the concept of abstraction and then go through the basic workflow of a targets pipeline. We will use plant trait data from two sites on Svalbard. One site is located close to nesting sites of sea birds receiving nutrient input while the other site is a reference location and has minimal nutrient input by sea birds.\nFollow the instructions below to download the repo containing data and code.\n\n\n\n\n\n\nExercise\n\n\n\nTo download the R project, run:\n\n#install.packages(\"usethis\") # if you don't have it already.\nusethis::use_course(\"biostats-r/targets_workflow_svalbard\")\n\nThen follow the instructions. This will open the targets-workflow-svalbard Rstudio project.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Before you start</span>"
    ]
  },
  {
    "objectID": "02-abstraction.html",
    "href": "02-abstraction.html",
    "title": "\n2  Functions and abstraction\n",
    "section": "",
    "text": "2.1 Long and complex code\nIn biology, code is used to manage, analyse and visualise data, and also writing reports and making presentations. Such code is often long and complex, consists of several parts that do different tasks and contains repetition. To run the full data analysis, the different pieces of code need to run sequentially. Managing and keeping track of the code is difficult and quickly becomes confusing.\nCode should be clear and communicate its purpose well. Keeping code transparent and reproducible, increasing the chance to be understood by others and your later self.\nOne way to break down long code into smaller steps and deal with the complexity is to use functions to abstract the complexity. Let’s talk about functions first.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Functions and abstraction</span>"
    ]
  },
  {
    "objectID": "02-abstraction.html#functions",
    "href": "02-abstraction.html#functions",
    "title": "\n2  Functions and abstraction\n",
    "section": "\n2.2 Functions",
    "text": "2.2 Functions\nA function is self contained code that accomplishes a specific task. Functions are very useful when a task is done several times. It saves repetition and makes the code more compact and clear. Functions can be called several times.\nR has many built-in functions that we use all the time. For example mean() which calculates the arithmetic mean.\nIt is also possible to write custom functions to do a specific task. In the next section we will explain how to make your own function.\n\n2.2.1 Make custom functions\nWe want to multiply two numbers, but the numbers are not always the same. This is a case to use a function.\nFunctions are made with the keyword function(), can have one or more arguments separated by commas, and need assigning to a name.\nFor a function that multiplies two numbers, we need two arguments, arg1 and arg2. We will name the function my_multiplier. It is advisable name the function with a meaningful name and not to use my_function.\n\nmy_multiplier &lt;- function(arg1, arg2){\n  arg1 * arg2\n}\n\nTo run the function, type the name of the function and set values for each argument inside brackets following the name of the function. The function will then return the result.\n\nmy_multiplier(arg1 = 3, arg2 = 4)\n\n[1] 12\n\n\n\n\n\n\n\n\nA function should do one task\n\n\n\nA function can be very simple or more complicated. But a function should do one task and not many tasks at the same time. Complicated code can be split into several functions.\n\n\nA more complicated function would be one that runs a linear regression. This function has three arguments: data, response and predictor. The function itself runs a linear regression and is named fit_model.\nTo run the function, type the name of the function and set all arguments. The output of the function can be stored as an object my_model.\n\nfit_model &lt;- function(data, response, predictor){\n    mod &lt;- lm(as.formula(paste(response, \"~\", predictor)), data = data)\n    mod\n}\n\nmy_model &lt;- fit_model(data = my_data, response = value, predictor = treatment)\nmy_model\n\n\n\n\n\n\n\nA function should be as general as possible\n\n\n\nFunctions should be made as general as possible, because this increases the chance to reuse the function. In the function above, we could have dropped the arguments response and predictor and hard coded value and treatment in the model. However, then this function could only be used for this specific dataset. By using response and predictor as arguments, we can reuse this function for any linear regression with one predictor.\nA function can even be reused in another analysis in a different R project. For this, the function has to be made into a R package (see the chapter Writing an R package in the R book).\n\n\n\n\n\n\n\n\nExercise\n\n\n\nImport and prepare the data\nGo to the targets-workflow-svalbard Rstudio project and open the trait_analysis.R file.\nWrite a function that cleans and prepares the data for analysis, covering line 12 to 22. Add the function in the functions.R file which is located in the folder called R. The file contains already one function.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Functions and abstraction</span>"
    ]
  },
  {
    "objectID": "02-abstraction.html#the-concept-of-abstraction",
    "href": "02-abstraction.html#the-concept-of-abstraction",
    "title": "\n2  Functions and abstraction\n",
    "section": "\n2.3 The concept of abstraction",
    "text": "2.3 The concept of abstraction\n“Abstraction should break down complex code chunks into smaller, self-explanatory tasks to better describe the purpose or the script” (Filazzola & Lortie, 2022).\nAbstraction splits up the code into different functions.\nThe main script contains little code, but is self explanatory, because all the details and complexity have been abstracted (removed). It imports the scripts and runs the functions\nThe file(s) containing the functions are sourced by the main script. Related functions can live in one file.\nHere is code that imports and cleans data, runs a model and produces a figure. Note that we are using a small example here to save space, but this code would likely be more complex in reality.\n\n### Script to test how marine nutrients affect leaf area in Alopecurus magellanicus\n\n# load libraries\nlibrary(tidyverse)\nlibrary(here)\nlibrary(performance)\n\n# import data\nraw_traits &lt;- read_delim(file = here(\"data/PFTC4_Svalbard_2018_Gradient_Traits.csv\"))\n\n# clean data and prepare for analysis\ntraits &lt;- raw_traits |&gt;\n  # remove NAs\n  filter(!is.na(Value)) |&gt;\n  # order factor and rename variable gradient\n  mutate(Gradient = case_match(Gradient,\n                               \"C\" ~ \"Control\",\n                               \"B\" ~ \"Nutrients\"),\n         Gradient = factor(Gradient, levels = c(\"Control\", \"Nutrients\"))) |&gt;\n  # select one species and one trait\n  filter(Taxon == \"alopecurus magellanicus\",\n         Trait == \"Leaf_Area_cm2\")\n\n# run a linear model\nmod_area &lt;- lm(Value ~ Gradient, data = traits)\nsummary(mod_area)\n# check model assumptions\ncheck_model(mod_area)\n\n# make figure\nggplot(traits, aes(x = Gradient, y = Value)) +\n  geom_boxplot(fill = c(\"grey80\", \"darkgreen\")) +\n  labs(x = \"\", y = expression(Leaf~area~cm^2)) +\n  theme_bw()\n\nThis is a long script and the code has to be studied very carefully to understand what is going on.\nLet’s abstract the code in a main script and some functions.\nThe main script:\n\n### Script to test how warming affects plant height in Bistorta vivipara.\n\n# load libraries\nlibrary(tidyverse)\nlibrary(here)\nlibrary(performance)\n\n# import data\nraw_traits &lt;- read_delim(file = here(\"data/PFTC4_Svalbard_2018_Gradient_Traits.csv\"))\n\n# clean data\ntraits &lt;- clean_data(raw_traits)\n\n# run model\nmy_model &lt;- fit_model(data = traits, \n                      response = \"Value\", \n                      predictor = \"Gradient\")\n# model summary\nsummary(my_model)\n# check model assumptions\ncheck_model(my_model)\n\n# plot treatments vs. plant height\nmy_figure &lt;- make_figure(traits)\n\nThe functions that are called in the main script are “hidden” in a separate script.\n\n### My custom functions\n\n# clean data\nclean_data &lt;- function(raw_traits){\n  traits &lt;- raw_traits |&gt;\n  # remove NAs\n  filter(!is.na(Value)) |&gt;\n  # order factor and rename variable gradient\n  mutate(Gradient = case_match(Gradient,\n                               \"C\" ~ \"Control\",\n                               \"B\" ~ \"Nutrients\"),\n         Gradient = factor(Gradient, levels = c(\"Control\", \"Nutrients\"))) |&gt;\n  # select one species and one trait\n  filter(Taxon == \"alopecurus magellanicus\",\n         Trait == \"Leaf_Area_cm2\")\n}\n\n# run a linear regression\nfit_model &lt;- function(data, response, predictor){\n  mod &lt;- lm(as.formula(paste(response, \"~\", predictor)), data = data)\n  mod\n}\n\n# make figure\nmake_figure &lt;- function(traits){\nggplot(traits, aes(x = Gradient, y = Value)) +\n  geom_boxplot(fill = c(\"grey80\", \"darkgreen\")) +\n  labs(x = \"\", y = expression(Leaf~area~cm^2)) +\n  theme_bw()\n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Functions and abstraction</span>"
    ]
  },
  {
    "objectID": "02-abstraction.html#resources",
    "href": "02-abstraction.html#resources",
    "title": "\n2  Functions and abstraction\n",
    "section": "\n2.4 Resources",
    "text": "2.4 Resources\n\nFilazzola, A., & Lortie, C. J. (2022). A call for clean code to effectively communicate science. Methods in Ecology and Evolution, 13(10), 2119-2128.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Functions and abstraction</span>"
    ]
  },
  {
    "objectID": "03-workflow.html",
    "href": "03-workflow.html",
    "title": "\n3  Getting started with targets\n",
    "section": "",
    "text": "3.1 Introduction to targets\nTargets is a pipeline tool, which coordinates the different steps in data science in R. It manages the workflow, takes care of dependencies in the code and keeps track of outdated objects. Targets takes care of the cash.\nA targets pipeline consists of different steps, such as importing data, running an analysis or making a figure (Figure 3.1) (1). Each step in the pipeline is a target and can for example be a data frame, a model or a figure. A target is basically an R object and is created by a function. A pipeline has a main script that puts all the pieces of code together and takes care of dependencies and keeping track of changes.\nThis concept should sound familiar to you after reading the previous chapter on abstraction.\nWhen updating and rerunning one part of the code, targets will skip parts where the upstream code has not changed and are therefore still up to date (Figure 3.1) (2). Targets will only rerun the code that is outdated and ensures that your results always match the underlying code and maintains a reproducible workflow (3). It avoids unnecessary repetition and can saves costly running time.\nFigure 3.1: Reproducible targets pipeline.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with targets</span>"
    ]
  },
  {
    "objectID": "03-workflow.html#introduction-to-targets",
    "href": "03-workflow.html#introduction-to-targets",
    "title": "\n3  Getting started with targets\n",
    "section": "\n3.1 Introduction to targets",
    "text": "3.1 Introduction to targets\nTargets is a pipeline tool, which coordinates the different steps in data science in R. It manages the workflow, takes care of dependencies in the code and keeps track of outdated objects. Targets takes care of the cash.\nA targets pipeline consists of different steps, such as importing data, running an analysis or making a figure (Figure 3.1) (1). Each step in the pipeline is a target and can for example be a data frame, a model or a figure. A target is basically an R object and is created by a function. A pipeline has a main script that puts all the pieces of code together and takes care of dependencies and keeping track of changes.\nThis concept should sound familiar to you after reading the previous chapter on abstraction.\n\n\n\n\n\n\nDefinitions\n\n\n\n\n\npipeline tool - coordinates different steps of data science\n\ntarget - an R object in memory\n\nfunction - self contained code that accomplish a specific task\n\n\n\nWhen updating and rerunning one part of the code, targets will skip parts where the upstream code has not changed and are therefore still up to date (Figure 3.1) (2). Targets will only rerun the code that is outdated and ensures that your results always match the underlying code and maintains a reproducible workflow (3). It avoids unnecessary repetition and can save costly running time.\n\n\n\n\nFigure 3.1: Reproducible targets pipeline.\n\n\n\n\n\n\n\n\n\nWhen is targets useful?\n\n\n\n\nWhen the code has a long runtime because it is slow or complex. Targets avoids running code that up to date and allows for parallel processing.\nWhen the workflow has interconnected tasks with dependencies"
  },
  {
    "objectID": "03-workflow.html#the-targets-pipeline",
    "href": "03-workflow.html#the-targets-pipeline",
    "title": "\n3  Getting started with targets\n",
    "section": "\n3.2 The targets pipeline",
    "text": "3.2 The targets pipeline\n\n3.2.1 The file structure\nA target workflow has a specific file structure including R code, functions, qmd files, data and a _targets.R file (Figure 3.2). The _targets.R file is mandatory and the most important file defining the targets pipeline. This file lives at the root of the R project folder.\n\n\n\n\nFigure 3.2: Minimal file structure of a target pipeline.\n\n\n\nAn R project will have many other files and it is recommended to keep code and data files in separate folders to keep the repository tidy. It is common to have one or several scripts that contain custom user-defined functions. These scripts should be stored in one folder. In this example we will call the folder R: R/functions.R.\nTo set up this file structure in an RStudio project, use the use_targets() function, which creates an initial _targets.R script with comments to help you populate the script.\n\n\n\n\n\n\nExercise\n\n\n\nGo to the targets-workflow-svalbard Rstudio project and load the targets library library(targets). Then start to set up a targets pipeline by using the use_targets() function.\n\n\n\n3.2.2 The _target.R file\nThe _targets.R file is the main script and configures and defines the pipeline. This file is mandatory and without it the targets pipeline will not work. When using the use_targets() function, it sets up the basic structure and comments to help fill out the rest (see below 👇) .\n\n# Created by use_targets().\n# Follow the comments below to fill in this target script.\n# Then follow the manual to check and run the pipeline:\n#   https://books.ropensci.org/targets/walkthrough.html#inspect-the-pipeline\n\n# Load packages required to define the pipeline:\nlibrary(targets)\n# library(tarchetypes) # Load other packages as needed.\n\n# Set target options:\ntar_option_set(\n  packages = c(\"tibble\") # Packages that your targets need for their tasks.\n  # format = \"qs\", # Optionally set the default storage format. qs is fast.\n  #\n  # Pipelines that take a long time to run may benefit from\n  # optional distributed computing. To use this capability\n  # in tar_make(), supply a {crew} controller\n  # as discussed at https://books.ropensci.org/targets/crew.html.\n  # Choose a controller that suits your needs. For example, the following\n  # sets a controller that scales up to a maximum of two workers\n  # which run as local R processes. Each worker launches when there is work\n  # to do and exits if 60 seconds pass with no tasks to run.\n  #\n  #   controller = crew::crew_controller_local(workers = 2, seconds_idle = 60)\n  #\n  # Alternatively, if you want workers to run on a high-performance computing\n  # cluster, select a controller from the {crew.cluster} package.\n  # For the cloud, see plugin packages like {crew.aws.batch}.\n  # The following example is a controller for Sun Grid Engine (SGE).\n  # \n  #   controller = crew.cluster::crew_controller_sge(\n  #     # Number of workers that the pipeline can scale up to:\n  #     workers = 10,\n  #     # It is recommended to set an idle time so workers can shut themselves\n  #     # down if they are not running tasks.\n  #     seconds_idle = 120,\n  #     # Many clusters install R as an environment module, and you can load it\n  #     # with the script_lines argument. To select a specific verison of R,\n  #     # you may need to include a version string, e.g. \"module load R/4.3.2\".\n  #     # Check with your system administrator if you are unsure.\n  #     script_lines = \"module load R\"\n  #   )\n  #\n  # Set other options as needed.\n)\n\n# Run the R scripts in the R/ folder with your custom functions:\ntar_source()\n# tar_source(\"other_functions.R\") # Source other scripts as needed.\n\n# Replace the target list below with your own:\nlist(\n  tar_target(\n    name = data,\n    command = tibble(x = rnorm(100), y = rnorm(100))\n    # format = \"qs\" # Efficient storage for general data objects.\n  ),\n  tar_target(\n    name = model,\n    command = coefficients(lm(y ~ x, data = data))\n  )\n)\n\n\n\n\n\n\n\nExercise\n\n\n\nOpen the _targets.R in your repo and have a look at your _targets.R file.\n\n\nThe _targets.R file has four main components.\n\nLoad necessary R packages\nDefine settings\nSource necessary functions\nWrite pipeline\n\nNote that the file also contains other options which are optional. We will discuss the four main component step-by-step.\n\nLoad the necessary R packages to get the pipeline started. This will mainly be the targets and maybe the tarchetypes package.\n\nThe tarchetypes package contains helper functions and needs to be loaded if your pipeline contains a quarto file (see next chapter).\n\n\ntar_option_set() sets all options such as load necessary packages or defining the output format. The argument package should have a list of all the required packages that are needed to run the pipeline. Note that targets needs to be loaded first and outside this function, otherwise the it cannot execute the tar_option_set() function.\n\nThe argument format let’s you define default storage format.\n\n# Load packages required to define the pipeline:\nlibrary(targets)\n# library(tarchetypes) # Load other packages as needed.\n\n# Set target options:\ntar_option_set(\n  packages = c(\"tibble\"), # packages that your targets need to run\n  format = \"rds\" # default storage format\n  # Set other options as needed.\n)\n\nNote that packages that are only used in a quarto file can be loaded directly in there and do not need to be loaded in the _targets.R file.\n\nThe function tar_source() will source all the R scripts in the environment of the pipeline.\n\n\n# Run the R scripts in the R/ folder with your custom functions:\ntar_source()\n\n\nIn the last section the pipeline is defined consisting of a list of targets. Each target is a step in the pipeline, for example importing data, run an analysis or make a figure and looks like a normal R object (e.g. tibble, vector, figure). Each target is declared by the tar_target() function and separated by a comma. The tar_target() needs two arguments: name defines the target name and command the code to produce the target. Usually the command is calling a custom function.\n\nHere is a target that uses the function fit_model() that was created in the previous chapter to run a linear regression.\n\n# fit model for plant height\nlist(\n  tar_target(name = model,\n             command = fit_model(data))\n  )\n\nEach target should have a unique name that can be called downstream in the pipeline. The order of the targets does not matter. The pipeline will figure out which targets depends on each other and run them in the right order.\nOnce the pipeline has run, the targets are stored.\n\n\n\n\n\n\nTargets names\n\n\n\nTargets names must be unique (no duplicates), should not start with a dot and the name should be meaningful (do not use my_variable).\n\n\nData files are special targets, because they also need the argument format to declare that this target is a file. Each time the pipeline is run, targetes will check if the file has been changed and if this is the case automatically import the data again the next time the pipeline is run.\n\nlist(\n  tar_target(name = file,\n             command = \"data/PFTC4_Svalbard_2018_Gradient_Traits.csv\",\n             format = \"file\")\n  )\n\n\n\n\n\n\n\nHow many targets should I make?\n\n\n\nA target should do one thing only (e.g. make a figure) and if a functions gets too long, it can be split into nested sub-functions to make the code readable and easier to maintain. Keep the number of targets manageable, which means keep a balance between the amount of code that goes in one target and the number of targets.\n\n\n\n3.2.3 Populate the _target.R file\nNow we can populate the _targets.R file. We need to set the options, source the custom functions, and define the pipeline.\nFirst, we need to add all R packages to the tar_option_set() function in the _targets.R file that are needed to run the pipeline.\n\n\n\n\n\n\nExercise\n\n\n\nAdd all the necessary R packages in tar_option_set() in the _targets.R file that are needed to run the pipeline.\n\n# Set target options:\ntar_option_set(\n  packages = c(\"tidyverse\") # packages that your targets need to run\n)\n\n\n\nThe next step is to source all the functions. The tar_source() function in the _targets.R file already does this. But we are missing one last function to make the figure.\n\n\n\n\n\n\nExercise\n\n\n\nWrite a function that makes a figure showing leaf area in both gradients.\n\n\nThe last step is to set up the targets pipeline.\n\n\n\n\n\n\nExercise\n\n\n\nDefine the data file\nAdd the dataset as data file in the _targets.R file.\n\nlist(\n  tar_target(name = file,\n             command = path,\n             format = \"file\")\n  )\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nAdd other targets to the pipeline\nAdd four more targets to the pipeline that\n\nimport the data\nclean the data\nrun a linear regression\nand make a figure\n\nThe last three targets are calling custom functions from the functions.R file. The targets are separated by a comma.\n\nlist(\n  # data file\n  tar_target(name = file,\n             command = path,\n             format = \"file\"),\n  \n  # import data\n  tar_target(name = target1,\n             command = function1),\n  \n  # clean data\n  tar_target(name = target2,\n             command = function2),\n  \n  ...\n  \n  )\n\n\n\nWell done, you have just set up your first target pipeline. Have a treat 🥕!\n\n3.2.4 Run the pipeline\nNow we are ready to run the pipeline. The function tar_make() runs the pipeline. This function looks for the _targets.R in the working directory and runs the pipeline. It makes sure the pipeline is run in the correct order.\nWhen running the pipeline for the first time you will see a list of all the targets that are dispatched and completed (Figure 3.3).\n\n\n\n\nFigure 3.3: R output after running the pipeline for the first time.\n\n\n\nAll targets are saved to the _targets/folder.\n\n\n\n\n\n\nExercise\n\n\n\nUse tar_make() to run the pipeline. Hopefully, everything will run smoothly 🤞! If not check out the Trouble shooting section below.\n\n\nWell done you have run your first targets pipeline. The tar_visnetwork() function shows the dependency graph of the pipeline and is a nice way of vizualizing the pipeline. Circles are targets, triangles functions, and the colour indicates if the targets are up to date or not.\n\n\n\n\nFigure 3.4: Vizualise the targets pipeline.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nRun tar_visnetwork() to visualize your pipeline.\n\n\nOnce the pipeline has run, it will always skip the targets that have not changed and are up to date and only run the once that need updating. In the long run this will save a lot of computational time and is one of the big advantages of using targets pipelines.\n\n\n\n\nFigure 3.5: R output after running the pipeline.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nChange something in your pipeline. For example filter for a different species: bistorta vivipara. Save the script and run tar_visnetwork() to visualize which object are not up to date now.\nRerun the pipeline using tar_make().\nAnd run tar_visnetwork() again to check if everything is up to date again.\n\n\n\n3.2.5 Load targets\nEach target can be loaded using tar_load() and the pipeline does not need rerunning each time before accessing the targets. This is a huge advantage of the targets pipeline and can save a lot of time.\n\ntar_load(model)\n\nTo access all the targets at once use tar_load_everything(). In tar_load() you can also use tidy select commands to load specific targets, e.g. tar_load(starts_with(\"y\"))\n\n\n\n\n\n\nExercise\n\n\n\nUse tar_load() to load the target traits.\nRun str(traits) to display the structure of the object traits.\n\n\n\n\n\n\n\n\nDo it step-by-step\n\n\n\nTargets plans can become huge and complex. Start small, create a few targets and functions and make the plan running. Then add new code in small steps and check regularly if the plan is still working This will help to understand and solve errors (see trouble shooting section).\n\n\n\n\nFigure 3.6: Vizualisation of complex target pipeline."
  },
  {
    "objectID": "03-workflow.html#trouble-shooting",
    "href": "03-workflow.html#trouble-shooting",
    "title": "\n3  Getting started with targets\n",
    "section": "\n3.3 Trouble shooting",
    "text": "3.3 Trouble shooting\n\n3.3.1 Locate problem\nError messages in targets can be hard to understand and will look something like this:\n\n\n\n\nFigure 3.7: Error in the pipeline.\n\n\n\nYou can see that the pipeline broke when running the target model. To learn more about the problem run the command targets::tar_meta(fields = error, complete_only = TRUE).\n\n\n\n\nFigure 3.8: Find out more about a problem in the pipeline.\n\n\n\nThe R output now gives some more information about the problem. Here the argument response is missing.\nIf an error is difficult to understand use tar_load() to load one specific target or tar_load_everything() to load all targets. Then you can try to run different parts of the script to test where the code breaks.\nBest practice is to run the pipeline regularly, which makes it easier locate where the error occurred.\n\n3.3.2 Object not found\nA common error is to call a target that does not exist. When running the pipeline this error will appear (Figure 3.9). This is usually if the name is spelled wrong or when using an old name.\n\n\n\n\nFigure 3.9: Error message for missing object\n\n\n\n\n3.3.3 Duplicate target\nAnother common mistake is to use the same name for two different targets (Figure 3.10). This is common when copy pasting code. Rename one of the objects and the problem is solved.\n\n\n\n\nFigure 3.10: Error message for duplicate target.\n\n\n\n\n3.3.4 Clean pipeline\nSometimes there are old targets in the pipeline that can cause problems. The tar_prune() function deletes data and metadata that are not used in the current pipeline anymore. The tar_delete() function deletes specific files that are stored in _targets/objects."
  },
  {
    "objectID": "03-workflow.html#resources",
    "href": "03-workflow.html#resources",
    "title": "\n3  Getting started with targets\n",
    "section": "\n3.4 Resources",
    "text": "3.4 Resources\n\nThe target manual contains everything you need to know and there is a large debugging section. \n\nHere is a large and working target plan\n\nA paper about pipelines: Brousil, M. R., Filazzola, A., Meyer, M. F., Sharma, S., & Hampton, S. E. (2023). Improving ecological data science with workflow management software. Methods in Ecology and Evolution. https://doi.org/10.1111/2041-210X.14113\n\nAnd finally, a short introduction to targets"
  },
  {
    "objectID": "04-output.html",
    "href": "04-output.html",
    "title": "\n4  Output files\n",
    "section": "",
    "text": "The results and figures of an analysis are usually presented in a report or presentation or both. One or more output files, such as a quarto document and presentations can be added to a targets pipeline.\nEach target from the pipeline can be loaded and used in the quarto document. The targets that are used in the quarto document need to be loaded into the current environment. For this we can use tar_load() or tar_read(). The first function is used when a target is used several times. tar_read() is useful if a target is only needed once, e.g. to show a figure.\n\n# print model output\ntar_load(model)\nsummary(model)\n\nAll R packages that are needed to run the quarto file need to be loaded in the .qmd file. If you are using a R package exclusively in the quarto script, the package can be loaded only in the quarto file, and does not need to be added to the _target.R file. targets always need to be loaded in both files.\nThe tarchetypes packages has to be loaded in the _target.R file. It has a number of useful helper functions including tar_quarto() which is needed to render quarto files in the targets pipeline.\n\n\n\n\n\n\nExercise\n\n\n\nOpen the target_quarto.qmd file.\nLoad and print the trait figure using tar_read() in the third code chunk (fig-trait-figure).\n\n\nTo add the quarto document to the pipeline, the manuscript has to be rendered. This is done in the _targets.R file in the list of targets using tar_quarto().\n\n# render ms\ntar_quarto(name = manuscript, path = \"target_quarto.qmd\")\n\n\n\n\n\n\n\nExercise\n\n\n\nAdd the quarto file to the pipeline\nCheck that you have loaded the tarchetypes package in the _target.R file.\nAdd tar_quarto() in the _targets.R file to render the quarto file.\nRun tar_make() to run the whole pipeline.\n\n\nVoilà, you have run your first targets pipeline including a quarto file. Have some 🍰!\n\n\n\n\n\n\nExercise\n\n\n\nDisplay figure Display the figure produced in the targets pipeline in the output file using tar_read(). There is a code chunk prepared to add this code.\nRun tar_make() to run the whole pipeline."
  }
]