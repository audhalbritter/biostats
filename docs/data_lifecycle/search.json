[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data collection",
    "section": "",
    "text": "1 Introduction\n\n\n\n\nFigure 1.1: ?(caption)\n\n\n\nData collection is the systematic gathering of data for a specific reason and the data is usually used for downstream processing in statistical analysis.\nIn this book, we will discuss a reproducible data management workflow. First, we will show you how to design spreadsheets for data collection. Second, we will discuss the whole life cycle of data, from data collection to sharing data. And finally, we have one chapter on how to best curate your data."
  },
  {
    "objectID": "01_design-spreadsheets.html#paper-or-digital-data-entry",
    "href": "01_design-spreadsheets.html#paper-or-digital-data-entry",
    "title": "2  Design spreadsheets",
    "section": "\n2.1 Paper or digital data entry",
    "text": "2.1 Paper or digital data entry\nThere are two ways data is collected. Data that is recorded from a machine and automatically stored. If you have the possibility for that, it is always recommended to make use of digital data collection. It can reduce errors. Alternatively, data is collected manually. There are multiple ways of collecting data by hand such as, on paper, digital on a notepad/phone, or recordings.\nEach of these methods have their advantages and disadvantages. If you are collecting data in remote place with harsh weather conditions, paper might be your only solution. Note that there exists rite in the rain paper. Digital data entry saves you the step of digitizing the data and therefore sources of errors. In addition, you can build in data validation for example drop down menus or a function that checks for the right range of values. This can be very useful for avoiding errors."
  },
  {
    "objectID": "01_design-spreadsheets.html#content",
    "href": "01_design-spreadsheets.html#content",
    "title": "2  Design spreadsheets",
    "section": "\n2.2 Content",
    "text": "2.2 Content\nWhat information should your spreadsheet contain? This is not en exhaustive list, but gives guidance on useful information to record:\n\nID (unique ID for each observation, individual)\nDate, time, observation number\nLocation: region/site\nExperimental design: block, plot, replicate, number of observation, treatments\nOrganism: species/population/genet\nresponse variable(s)\npredictors\nrecorder/scribe\nother observations: weather\nnotes"
  },
  {
    "objectID": "01_design-spreadsheets.html#data-validation-tools",
    "href": "01_design-spreadsheets.html#data-validation-tools",
    "title": "2  Design spreadsheets",
    "section": "\n2.3 Data validation tools",
    "text": "2.3 Data validation tools\nData validation is a way to reduce errors in the data and can be built in when collecting or digitizing data.\nFor example you can:\n\nset ranges for valid numbers (e.g. only positive, range between two numbers)\nonly allow whole numbers or decimals\nadd a drop down menus for categorical data\ndefine the length of text (e.g. only 8 characters)\ndefine data types (e.g. to avoid conversion to dates)\n\n\n\n\n\nFigure 2.2: Video"
  },
  {
    "objectID": "01_design-spreadsheets.html#rectangular-spreadsheet",
    "href": "01_design-spreadsheets.html#rectangular-spreadsheet",
    "title": "2  Design spreadsheets",
    "section": "\n2.4 Rectangular spreadsheet",
    "text": "2.4 Rectangular spreadsheet\nSpreadsheets should be rectangular. Best practice is to make spreadsheets completely rectangular. They should not have empty cells, rows or columns, titles or double headers.\nIt is however common to leave some rows/columns and cells empty (Figure 2.3). Also adding a title to a spreadsheet is often done. It is not best practice, but also not a big problem for the downstream processing. Contrary, having two headers with different information is more difficult to process later.\n\n\n\n\nFigure 2.3: An almost rectangular and tidy dataset. The note column has empty cells.\n\n\n\nSometimes, two datasets are combined in one spreadsheet. For example, table Figure 2.4 shows an example of pollinator observation data, which includes observations about wind. The spreadsheet also contains a separate table on the right side, showing the scale for wind. It can be useful to have the two tables in the same file, for example when entering the data. But this will be far more complicated than needed when importing the data. Here we recommend to keep these two tables in separate spreadsheets.\n\n\n\n\nFigure 2.4: Bad examples of spreadsheets. A) Shows two different datasets merged into one. B) Shows a series of small tables belonging to the same dataset."
  },
  {
    "objectID": "01_design-spreadsheets.html#long-or-wide-format",
    "href": "01_design-spreadsheets.html#long-or-wide-format",
    "title": "2  Design spreadsheets",
    "section": "\n2.5 Long or wide format",
    "text": "2.5 Long or wide format\nDatasets can be long or wide and there is often a debate which of these formats are better (Figure 2.5). We do not have a strong opinion on this. As long as the general rules (see above and below) are followed, this does not matter very much. Many analysis require a long format, but for others (e.g. ordinations) a wide format is needed. This means that data often needs to be reshaped from long to wide and vice versa. And this is not very difficult (see reshape section).\n\n\n\n\nFigure 2.5: Wide (A) and long (B) data table."
  },
  {
    "objectID": "01_design-spreadsheets.html#single-value-per-cell",
    "href": "01_design-spreadsheets.html#single-value-per-cell",
    "title": "2  Design spreadsheets",
    "section": "\n2.6 Single value per cell",
    "text": "2.6 Single value per cell\nTidy spreadsheets follow the following rules:\n\neach variable should be one specific column,\neach observation should be one specific row,\neach cell at the intersection of a row and a column contains a single value.\n\nImportantly, put only one value per cell (Figure 2.3).\nSometimes values are entered with their units, such as 42 g in one cell (Figure 2.6). It is better to separate the value and the unit into two columns.\nAnother common mistake is to add notes to a column (Figure 2.6). For example if a value is 0 because it is below the detection value, you could write 0 (below threshold). We recommend to only write the number in the first column and add notes on the value in a separate column called notes.\n\n\n\n\nFigure 2.6: Wide (A) and long (B) data table."
  },
  {
    "objectID": "01_design-spreadsheets.html#consistency",
    "href": "01_design-spreadsheets.html#consistency",
    "title": "2  Design spreadsheets",
    "section": "\n2.7 Consistency",
    "text": "2.7 Consistency\nConsistency is key. There are many ways of designing a spreadsheet, and there is not always a right or a wrong. Find what works for you and stick to it.\nBe consistent for categorical variables, for example use the same spelling and not variations like: female, Female, F. Latin species names is a common problem and where typos happen very easily (Figure 2.7).\n\n\n\n\nFigure 2.7: Inconsistency in species names\n\n\n\nIf you have multiple files or datasets from the same experiment, be consistent with variable names and do not use variations like: site, location, siteID. This will make it more difficult to join datasets downstream.\nBe consistent with missing values. Do not use a mix of leaving the cell blank, NA and making notes like value missing. Also see section below for more details on missing values.\nBe consistent with file names.\nBe consistent with dates. Dates are particularly tricky and get some special attention here (see below). Preferably, use the ISO standard yyyy-mm-dd, for example 2024-11-05.\nBe consistent in notes. We recommend to have a column with notes, which can be used to write down notes about the data or a specific value. For example, why a observation is missing, or something that was unusual during data collection etc. But again, be consistent when making these notes, because it will be easier to make use of the notes downstream. Using different versions for the same information like gone, missing, vanished will make it difficult to quantify how many times a specific problem occured.\nAvoid space in cells before ” female” or after “female ”."
  },
  {
    "objectID": "01_design-spreadsheets.html#meaningful-naming",
    "href": "01_design-spreadsheets.html#meaningful-naming",
    "title": "2  Design spreadsheets",
    "section": "\n2.8 Meaningful naming",
    "text": "2.8 Meaningful naming\nUse good and meaningful names. What do we mean by this? Variable names should be easy to use in downstream data analysis. In addition, variable names should be meaningful which means that the name should explain the variable to some extent.\nAvoid spaces in names, and rather use underscore (_) or hyphen (-). There are different styles (see Figure 2.8) and there are debates about which one is preferable. The truth is, it does not really matter, choose one and stick with it. On a second thought, do not use the kebab-case.\n\n\n\n\nFigure 2.8: Different styles for naming objects. Credit: Allison Horst.\n\n\n\nDo not use special characters other than underscore and hyphen in names. For example: +, %, &, /, ?, !, $, ,, #, @. Note that letters that might be common for you, for example å, æ and ø, are not so common in other countries and R does not deal very well with them. Instead of nedbør use nedbor. This will make you life a lot easier.\nUse concise and meaningful names (Figure 2.9). The name should be short, but long enough to give a meaning. For example community_composition_2022.csv.\n\n\n\n\nFigure 2.9: Final doc by PhDcomics.com\n\n\n\nIf you want to know more about this, we have created a tutorial on naming conventions.\n\n2.8.1 janitor\nOne way to deal with inconsistent variable names is to use the janitor package. The function clean_names() will convert all variables names to a consistent format with snake_case (default). Other formats are also available. In addition it will turn % sign into percent and # to number.\nLet’s look at a dataframe with ugly variable names. And then clean the names.\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\ndat\n\n# A tibble: 1 × 3\n  siteID `measurment 2022` `cover%`\n  &lt;chr&gt;              &lt;dbl&gt;    &lt;dbl&gt;\n1 A                      1       32\n\ndat |&gt; \n  clean_names()\n\n# A tibble: 1 × 3\n  site_id measurment_2022 cover_percent\n  &lt;chr&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n1 A                     1            32"
  },
  {
    "objectID": "01_design-spreadsheets.html#standards",
    "href": "01_design-spreadsheets.html#standards",
    "title": "2  Design spreadsheets",
    "section": "\n2.9 Standards",
    "text": "2.9 Standards\nUse global data standards when available.\ngeographic locations\nOne example are geographic locations such as coordinates. They can be written in many different ways:\n\nDecimal degrees: 60.39299 5.32415\nDMS: 60°23’34.76” N 5°19’26.94” E\nUTM: 32V 297477 6700830\n\nWe recommend to use decimal degree (ISO 6709). For example the coordinates for Bergen (Norway) are 60.39299 °N and 5.32415 °E. There are some important rules to follow:\n\nLatitude comes before longitude.\nNorth latitude is positive and south latitude is negative.\nEast longitude is positive and west longitude is negative.\n\nIf your coordinates are UTM that is also fine, but do not forget to report the zone (e.g. 32V).\nWhen entering dates into spreadsheets, add each part in a separate column, meaning separate the degree north and east in two columns. For UTM use three columns.\ndates\nAnother example are dates, that can also be written in many different ways:\n\n3.1.2022\n03/01/2200\n01-03-2022\n\nThis can lead to confusion, especially when the placement of the month and day are switched (Figure 2.10).\nWe therefore strongly recommend to use the global standard ISO 8601 (Houston 1993) for dates, YYYY-MM-DD, such as 2024-11-05.\n\n\n\n\nFigure 2.10: Missunderstandings when not using date standards. Credit: https://xkcd.com\n\n\n\nAnother issue with dates is, that programs like Excel can turn names into dates. For example names like “Oct-4”, which is a name of a gene, will be turned into a date. Be aware of such problems and check your dataset for dates that are not supposed to be dates. This is a widely ackowledged problem and there are tools to deal with it, such as the web tool that autocorrects and updates misidentified gene names (Koh et al. 2022).\nTry to avoid names that are turned into dates, or actively prevent it by adding a underscore or another character in front of such names. This can later be removed. Another way is to force a column in excel to be a date or to be text."
  },
  {
    "objectID": "01_design-spreadsheets.html#missing-data",
    "href": "01_design-spreadsheets.html#missing-data",
    "title": "2  Design spreadsheets",
    "section": "\n2.10 Missing data",
    "text": "2.10 Missing data\nWe recommend to use NA for missing data and not to leave cells blank. R will automatically fill in NAs in blank cells. The problem with leaving missing data blank, is that there is no way of distinguishing between actual missing data and data that was forgotten to enter."
  },
  {
    "objectID": "01_design-spreadsheets.html#point-or-comma-as-decimal-separator",
    "href": "01_design-spreadsheets.html#point-or-comma-as-decimal-separator",
    "title": "2  Design spreadsheets",
    "section": "\n2.11 Point or comma as decimal separator",
    "text": "2.11 Point or comma as decimal separator\nFor the decimal separator it is common to use a point or a comma. There is no general agreement on which one to use, and there are different practises in different countries. For example in Norway, the comma is the standard setting in Excel.\nAgain, it does not matter what you use, but use it consistently. We have guidelines for how to import datasets with different formats (see import chapter).\nImportantly, do not use a point, when Excel is expecting a comma, which can cause problems when importing the data to R. You can change the settings in Excel."
  },
  {
    "objectID": "01_design-spreadsheets.html#no-manipulation-or-calculations",
    "href": "01_design-spreadsheets.html#no-manipulation-or-calculations",
    "title": "2  Design spreadsheets",
    "section": "\n2.12 No manipulation or calculations",
    "text": "2.12 No manipulation or calculations\nWe strongly recommend to use datasheets only for data entry and storage. During data entry, the content of the spreadsheet can still be changed to reflect the paper version of the data or to make it consistent. But at some point we suggest to stop data manipulation by hand. From that point, data manipulation is done by code.\nThis is because any manipulation done by hand in Excel, cannot be reversed (at least once the document is saved and closed). Also, if you do a lot of manipulation in Excel, you won’t remember what has been done. Standard excel does not have track change to allow you to go back to older versions. In contrast, if data manipulation is done code-based, all manipulation can be changed and reversed.\nFor example, if you delete a column in Excel, save the document and leave the program. And the next day you realize that this was a mistake, it is not possible to retrieve this column. However if you leave the data file untouched, and do the manipulation in R, you can import the data again, run the code and just delete the command that was wrong.\nWe also do not recommend that you make calculations or summaries in your spreadsheet. Better practice is to use R for this, because it is reproducible."
  },
  {
    "objectID": "01_design-spreadsheets.html#no-formatting",
    "href": "01_design-spreadsheets.html#no-formatting",
    "title": "2  Design spreadsheets",
    "section": "\n2.13 No formatting",
    "text": "2.13 No formatting\nIt’s common to format tables, by for example using colours, or bold font. That is fine, as long as the formatting is not containing any information. For example colouring missing data (Figure 2.11). For that make a new column with notes that data is missing.\nData analysis programs do not understand highlighted cells or bold text and such information will simply be ignored and is therefore lost.\n\n\n\n\nFigure 2.11: A spreadsheet with formatting."
  },
  {
    "objectID": "01_design-spreadsheets.html#references",
    "href": "01_design-spreadsheets.html#references",
    "title": "2  Design spreadsheets",
    "section": "\n2.14 References",
    "text": "2.14 References\n\n\n\n\n\n\nBroman, Karl W, and Kara H Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10.\n\n\nHouston, Gary. 1993. “ISO 8601: 1988 Date/Time Representations.”\n\n\nKoh, Clara WT, Justin SG Ooi, Gabrielle LC Joly, and Kuan Rong Chan. 2022. “Gene Updater: A Web Tool That Autocorrects and Updates for Excel Misidentified Gene Names.” Scientific Reports 12 (1): 1–7."
  },
  {
    "objectID": "02_data_workflow.html#digitizing-data",
    "href": "02_data_workflow.html#digitizing-data",
    "title": "\n3  Data workflow\n",
    "section": "\n3.1 Digitizing data",
    "text": "3.1 Digitizing data\nThe first step after data collection is to digitize the data, unless the data was collected digitally. When digitizing the data, the digital version should reflect the paper version, to avoid having to flip to different sections of the spreadsheet. Do not change the format, just to have a long table. This can easily be changed later."
  },
  {
    "objectID": "02_data_workflow.html#proof-reading",
    "href": "02_data_workflow.html#proof-reading",
    "title": "\n3  Data workflow\n",
    "section": "\n3.2 Proof reading",
    "text": "3.2 Proof reading\nAfter digitizing your data, the next step is to proof read. Proof reading basically means that you make the digital copy of your data reflect the paper copy. At this stage, the digital spreadsheet can still be edited by hand, if you are correcting for typos and similar issues.\nNote that if you discover consistent error (e.g. entering the wrong date for multiple days), it is safer to make these changes using code.\nAfter proof reading we recommend to save your data as raw data, a non-manipulated version of your data. Indicate in file name, that this is the raw version of the data. This will later make the process of data cleaning fully transparent. For example, if you want to share your data later, anybody can see what was done to the data, and if somebody ever wants to clean your data differently for another purpose, this is still possible."
  },
  {
    "objectID": "02_data_workflow.html#data-cleaning",
    "href": "02_data_workflow.html#data-cleaning",
    "title": "\n3  Data workflow\n",
    "section": "\n3.3 Data cleaning",
    "text": "3.3 Data cleaning\nData cleaning is the process of detecting and correcting or removing, incomplete, incorrect, irrelevant, duplicated or improperly formatted data from a dataset. Errors and problems in the data can be a problem or limit the downstream data analysis and affect the results. Therefore, data cleaning can solve some of the problems and improve the data, analysis and their outcome.\nThe data cleaning should be done fully code-based, meaning that from now on, there should be no more changing things in the data by hand. Make sure your code is openly available (e.g. on GitHub) to make the data cleaning workflow transparent and reproducible. For more details see section on data cleaning (Chapter 4).\nAfter data cleaning, we recommend to save a clean version of your data and indicate again in the file name that this is the clean version."
  },
  {
    "objectID": "02_data_workflow.html#storage",
    "href": "02_data_workflow.html#storage",
    "title": "\n3  Data workflow\n",
    "section": "\n3.4 Storage",
    "text": "3.4 Storage\nSave your data as plain text formats, with comma or tab deliminator. We recommend csv files, which is a nonproprietary format. This means that accessing the file does not require any special software and can be opened in any spreadsheet program."
  },
  {
    "objectID": "02_data_workflow.html#backup",
    "href": "02_data_workflow.html#backup",
    "title": "\n3  Data workflow\n",
    "section": "\n3.5 Backup",
    "text": "3.5 Backup\nBack up your data in an data repository that is not connected to your computer to avoid data loss. There are many options for this and they often have private or closed repositories, which means that you do not automatically need to share the data.\nTake pictures of paper versions of your data and store them in a save place. Pictures can be useful for proof reading."
  },
  {
    "objectID": "02_data_workflow.html#data-dictionary-and-documentation",
    "href": "02_data_workflow.html#data-dictionary-and-documentation",
    "title": "\n3  Data workflow\n",
    "section": "\n3.6 Data dictionary and documentation",
    "text": "3.6 Data dictionary and documentation\nA dataset on its own is often useless, because it can be difficult to understand what data each column contains. Therefore, it is best practise to create a data dictionary that goes with a dataset (Figure 3.2). A data dictionary is a separate file or table that describes the data and explains what each variable means.\nThis is useful if you want to share your data with collaborators, if somebody else is doing the data analysis for you, or even for your future self in a few years time. You might not remember exactly what each column means, the units, and how it was measured.\nA data dictionary should contain:\n\nVariable names\nExplanation of the variable\nUnit\nRange/expected min/max\n\n\n\n\n\nFigure 3.2: Example of a data dictionary\n\n\n\nAnother important part of a dataset is the documentation or metadata of how the data was collected. This can be a protocol, a method section in a paper or even a data paper. What option you choose is up to you. It is however important that when sharing the data, the user has access to this information."
  },
  {
    "objectID": "02_data_workflow.html#version-control",
    "href": "02_data_workflow.html#version-control",
    "title": "\n3  Data workflow\n",
    "section": "\n3.7 Version control",
    "text": "3.7 Version control\nwill be added soon."
  },
  {
    "objectID": "03_data_curation.html#useful-packages",
    "href": "03_data_curation.html#useful-packages",
    "title": "4  Data cleaning",
    "section": "\n4.1 Useful packages",
    "text": "4.1 Useful packages\nThere are a couple of R packages that are useful for data curation. First, tidyverse is a collection of R packages used for basic data manipulation and analysis. We will also use lubridate, which helps with data and time formats.\nIf you have never used the packages you need to install it first using the function install.packages(\"tidyverse\"). Otherwise, you can just load the packages.\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\nAnother useful package for data curation is tidylog, which is built on the dplyr and tidyr packages and provides useful information about the functions used.\nTidylog will for example tell you how many rows have been removed and are remaining when using the filter() function or how many rows match when using a join function. The information is always indicated in absolute numbers and percentage. This additional information is very useful to check if the right observations have been removed or manipulated, because mistakes are easily done.\nLet’s install and/or load tidylog.\n\nlibrary(tidylog)\n\nNote, that once tidylog is loaded it will automatically prioritize the tidylog function before the dplyr and tidyr functions. You actively have to choose if you do not want to use the tidylog version by using this notation: dplyr::filter().\nSome data checking has to be done by hand and detecitve work, other things can be done more automatically. There are a few packages that can help with some of this work and for this tutorial we will use the validate package.\n\n#install.packages(\"validate\")\nlibrary(validate)"
  },
  {
    "objectID": "03_data_curation.html#import-data",
    "href": "03_data_curation.html#import-data",
    "title": "4  Data cleaning",
    "section": "\n4.2 Import data",
    "text": "4.2 Import data\nThe first step is to import the dataset to R. The data is stored as a csv file and we can use the function read_csv() to import that data. If your data has another format or you are new to importing data, have a look at this page.\nGive the dataset a name using a backwards pointing arrow: &lt;- The name should indicate that this is the raw data.\n\nraw_traits &lt;- read_csv(\"data/PFTC4_Svalbard_2018_Gradient_Traits.csv\")\n#&gt; Rows: 11345 Columns: 15\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr  (7): Project, Gradient, PlotID, ID, Functional_group, Taxon, Trait\n#&gt; dbl  (7): Year, Site, Individual_nr, Value, Elevation_m, Latitude_N, Longitu...\n#&gt; date (1): Date\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nThe dataset has 11345 rows and 15 columns and a number of numeric, character and date variables. It contains measurements of 14 traits from two elevational gradients on Svalbard. The traits were measured on individual plants from 21 different graminoid and forb species. For more information about the sites, traits and measurements see here.\nSome manipulation\nLet us introduce some errors to the dataset.\nThe code to do this is hidden. But if you want to replicate the code to introduce errors you can find the code from line 116."
  },
  {
    "objectID": "03_data_curation.html#view-dataset",
    "href": "03_data_curation.html#view-dataset",
    "title": "4  Data cleaning",
    "section": "\n4.3 View dataset",
    "text": "4.3 View dataset\nFirst, we want to have a look at the dataset. By typing raw_traits in the console it will display the first rows and columns of the dataset. Note that the last column and many rows are not shown.\n\nraw_traits\n#&gt; # A tibble: 10,298 × 10\n#&gt;    Date       Gradient  Site PlotID Individual_nr ID      Taxon    Trait   Value\n#&gt;    &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt;\n#&gt;  1 2018-07-20 B            5 D                  1 AIB1395 saxifra… Plan… 6.5 e+0\n#&gt;  2 2018-07-20 B            5 D                  1 AIB1395 saxifra… Wet_… 2.92e-2\n#&gt;  3 2018-07-20 B            5 D                  1 AIB1395 saxifra… Dry_… 4   e-3\n#&gt;  4 2018-07-20 B            5 D                  1 AIB1395 saxifra… Leaf… 5.66e-1\n#&gt;  5 2018-07-20 B            5 D                  1 AIB1395 saxifra… Leaf… 6.75e-1\n#&gt;  6 2018-07-20 B            5 D                  1 AIB1395 saxifra… SLA_… 1.69e+2\n#&gt;  7 2018-07-20 B            5 D                  1 AIB1395 saxifra… LDMC  1.37e-1\n#&gt;  8 2018-07-20 B            5 D                  1 AIB1395 saxifra… C_pe… 3.89e+1\n#&gt;  9 2018-07-20 B            5 D                  1 AIB1395 saxifra… N_pe… 1.14e+0\n#&gt; 10 2018-07-20 B            5 D                  1 AIB1395 saxifra… CN_r… 3.41e+1\n#&gt; # ℹ 10,288 more rows\n#&gt; # ℹ 1 more variable: Elevation_m &lt;dbl&gt;\n\nAt the top you can see that the dataset has 10298 observations and 10 columns. These numbers give you a first indication if you have imported the right dataset, and if all observations and columns are there."
  },
  {
    "objectID": "03_data_curation.html#check-variable-type",
    "href": "03_data_curation.html#check-variable-type",
    "title": "4  Data cleaning",
    "section": "\n4.4 Check variable type",
    "text": "4.4 Check variable type\nOne of the things we want to do is checking if all the variables in the dataset have the right type. For each variable the output above indicates the data type just below the variable name. The most common types are dbl (numeric or integer), chr (character), or date (date).\nIf you are unfamiliar with data types see here.\nThe first variable Date is a character, which does not seem to be correct. This means that one or several observations in this column are not dates. Since we do not expect to have very many dates (the data was collected during a few days), we can check all the different values in Date. For this we use the function distinct() on the variable Date.\n\nraw_traits |&gt; \n  distinct(Date)\n#&gt; distinct: removed 10,291 rows (&gt;99%), 7 rows remaining\n#&gt; # A tibble: 7 × 1\n#&gt;   Date      \n#&gt;   &lt;chr&gt;     \n#&gt; 1 2018-07-20\n#&gt; 2 2018-07-18\n#&gt; 3 2018-07-21\n#&gt; 4 2018-07-19\n#&gt; 5 2018-07-17\n#&gt; 6 18        \n#&gt; 7 &lt;NA&gt;\n\nWe see that there are 6 distinct dates in the variable Date. One of the dates is “18”, which is not a correct date format and turned the variable into a character. Note the additional information from the tidylog package on the distinct() function, which shows the number of rows removed and remaining.\nThe next step is to check which observastion(s) have the wrong date. For this we can use the function filter() to extract all observations with the date 18. We can pipe this to View(), which will display the whole table in a separate window. Note that for this tutorial, we use a different way of displaying the output\n\nraw_traits |&gt; \n  filter(Date == \"18\") |&gt; \n  View()\n\n\n#&gt; filter: removed 10,291 rows (&gt;99%), 7 rows remaining\n#&gt; # A tibble: 7 × 10\n#&gt;   Date  Gradient  Site PlotID Individual_nr ID      Taxon         Trait    Value\n#&gt;   &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;    &lt;dbl&gt;\n#&gt; 1 18    C            1 A                  3 AMO3822 salix polaris Plant… 1.1 e+0\n#&gt; 2 18    C            1 A                  3 AMO3822 salix polaris Wet_M… 5.76e-3\n#&gt; 3 18    C            1 A                  3 AMO3822 salix polaris Dry_M… 2   e-3\n#&gt; 4 18    C            1 A                  3 AMO3822 salix polaris Leaf_… 1.88e-1\n#&gt; 5 18    C            1 A                  3 AMO3822 salix polaris Leaf_… 2.84e-1\n#&gt; 6 18    C            1 A                  3 AMO3822 salix polaris SLA_c… 1.42e+2\n#&gt; 7 18    C            1 A                  3 AMO3822 salix polaris LDMC   3.47e-1\n#&gt; # ℹ 1 more variable: Elevation_m &lt;dbl&gt;\n\nWe can see that a single observation (with multiple traits) has the wrong date. There is no way that we would remember on what day this leaf was collected (remember the dataset has &gt; 10000 leaves!). We have to start playing detectives now. The only way to find the right date for these observations is to check the raw data (leaves), notes or photos. It is therefore important to keep all the data entry sheets (paper version), take a photo of them and make tidy notes during field work. This is the only way to fix many of the problems.\nLuckily, we took pictures from all envelopes of the leaves. The date on the envelope is 18 July 2018 (Figure 4.1), and it seems that there has been a typo.\n\n\n\n\nFigure 4.1: The envelope of the leaf with the wrong date.\n\n\n\nLet’s replace the wrong date and give the variable Date the right class.\nFor this we will use the function mutate() which adds or manipulates a column. Inside the mutate we will use the if_else() function to replace the date for a specific ID. This function is useful for a single condition. However for multiple statements (many if else conditions), we recommend to use the case_when() function (see below). To change the class, we use the ymd() function from the lubridate package. Note that we now have to assign the table to a new or the same name to make the change permanent.\n\nraw_traits &lt;- raw_traits |&gt; \n  mutate(Date = if_else(ID == \"AMO3822\", \"2018-07-18\", Date)) |&gt; \n  mutate(Date = ymd(Date))\n#&gt; mutate: changed 7 values (&lt;1%) of 'Date' (0 new NAs)\n#&gt; mutate: converted 'Date' from character to Date (0 new NA)\n\nAn important step and good practice when cleaning data is to check that the right correction has been done. Here is where the tidylog package comes in handy. It shows that for 7 observation Date has been changed. This matches with the number of observations that had a wrong date.\nTo be absolutely sure we can look at the specific leaf (ID == “AMO3822”) and see if the date is now corrected. Another way would be to run the distinct(Date) function again.\n\nraw_traits |&gt; \n  filter(ID == \"AMO3822\") |&gt; \n  select(Date)\n#&gt; filter: removed 10,291 rows (&gt;99%), 7 rows remaining\n#&gt; select: dropped 9 variables (Gradient, Site, PlotID, Individual_nr, ID, …)\n#&gt; # A tibble: 7 × 1\n#&gt;   Date      \n#&gt;   &lt;date&gt;    \n#&gt; 1 2018-07-18\n#&gt; 2 2018-07-18\n#&gt; 3 2018-07-18\n#&gt; 4 2018-07-18\n#&gt; 5 2018-07-18\n#&gt; 6 2018-07-18\n#&gt; 7 2018-07-18\n\nThe date has been fixed.\n\n\n\n\n\n\nExercise 1\n\n\n\nNow it is your turn. Check if the data type for the variable Date is now correct.\nHint\n\ntype raw_traits to look at the whole dataset where the datatype of each variable is indicated\nuse class(raw_traits$Date) which will tell you directly what type of class the variable has\nuse map(raw_traits, class) to get the class of all variable in the dataframe\n\n\n\nUsing the validate package, we can check all the variables at once. The validate package is based on making some rules that need checking and then applying those rules to a dataset. The rules can be reused and applied to any dataset.\nLet’s make some rules about data types using the validator() function:\n\n# rules\nrules &lt;- validator(\n  \n  # check variable types\n  is.character(Gradient),\n  is.character(PlotID),\n  is.character(ID),\n  is.character(Taxon),\n  is.character(Trait),\n\n  is.numeric(Site),\n  is.numeric(Individual_nr),\n  is.numeric(Value),\n  is.numeric(Elevation_m),\n  \n  is.Date(Date))\n\nNow the rules can be applied to the dataset using the confront() function.\n\nout &lt;- confront(raw_traits, rules)\nsummary(out)\n  \n\nThe summary function gives an overview of each rule and how many passes and fails there are. It looks like all the rules are passed."
  },
  {
    "objectID": "03_data_curation.html#check-for-duplicates",
    "href": "03_data_curation.html#check-for-duplicates",
    "title": "4  Data cleaning",
    "section": "\n4.5 Check for duplicates",
    "text": "4.5 Check for duplicates\nAnother common problem is duplicate observations. This can happen when data is entered twice. The why to find duplicates is to check that the combination of variables are unique. In our dataset, we expect that Date, Gradient, Site, PlotID, Individual_nr, ID, Taxon and Trait should be unique, and only occurring once.\nTo check this, we can use the rule is_unique().\nNote that Value was not included in the is_unique(). This was done intentionally, because a common mistake is to have a duplicate, but with a different value. This is either because one of the variables is wrong, e.g. it has the wrong Site and therefore appears to be a duplicate. Alternatively, the leaf could have been measured twice by accident, which would likely give two slightly different values. When getting a duplicate, these different options for why there is a duplicate have to be considered and carefully checked in the raw data.\n\nrules &lt;- validator(\n  \n  # check variable types\n  is_unique(Date, Gradient, Site, PlotID, Individual_nr, ID, Taxon, Trait))\n\nout &lt;- confront(raw_traits, rules)\nsummary(out)\n#&gt;   name items passes fails nNA error warning\n#&gt; 1   V1 10298  10296     2   0 FALSE   FALSE\n#&gt;                                                                 expression\n#&gt; 1 is_unique(Date, Gradient, Site, PlotID, Individual_nr, ID, Taxon, Trait)\n\nTwo observations fail the rules and are not unique.\nThe violate package can visualise the number of passes and fails, which can be useful. For this, use the plot function.\n\nplot(out)\n\n\n\nA plot showing the fails and passes. Note that there are very few fails compared to passes and the red bar is not visible on the plot.\n\n\n\nTo fix the problem, we need to know which observation is a duplicate. Here, we can use the function violating() for the data and the results and it will show the duplicate rows.\n\nviolating(raw_traits, out)\n#&gt; # A tibble: 2 × 10\n#&gt;   Date       Gradient  Site PlotID Individual_nr ID      Taxon     Trait   Value\n#&gt;   &lt;date&gt;     &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;\n#&gt; 1 2018-07-20 B            3 C                  3 BEK3638 salix po… Dry_… 0.00275\n#&gt; 2 2018-07-20 B            3 C                  3 BEK3638 salix po… Dry_… 0.00275\n#&gt; # ℹ 1 more variable: Elevation_m &lt;dbl&gt;\n\nWe get two exact duplicates, where even Value is the same. We can therefore assume that the leaf has only been measured once, but the data has been entered twice.\nTo fix the problem, we want to remove one of the duplicates. We group by the variables we expect to be unique and use distinct() with the argument .keep_all = TRUE to remove the duplicates.\n\nraw_traits &lt;- raw_traits |&gt; \n  group_by(Date, Gradient, Site, PlotID, Individual_nr, ID, Taxon, Trait) |&gt; \n  distinct(.keep_all = TRUE) |&gt; \n  ungroup()\n#&gt; group_by: 8 grouping variables (Date, Gradient, Site, PlotID, Individual_nr, …)\n#&gt; distinct (grouped): removed one row (&lt;1%), 10,297 rows remaining (removed 0 groups, 10,297 groups remaining)\n#&gt; ungroup: no grouping variables remain\n\nTidylog shows again what happens and how many rows have been removed. There are 8 grouping variables and as expected, one row is removed, which is the duplicated row.\nWe can also run the code from above again to check if the duplicate is gone."
  },
  {
    "objectID": "03_data_curation.html#check-for-missing-data",
    "href": "03_data_curation.html#check-for-missing-data",
    "title": "4  Data cleaning",
    "section": "\n4.6 Check for missing data",
    "text": "4.6 Check for missing data\nA common problem in a dataset are missing data. There are many reasons for having missing data. For now, we want to find out if we have any NAs in the dataset and if yes where and how many.\nA quick way to get an overview of all NAs in the dataset is to select for any NAs in the dataset and summarise how many NAs there are.\n\nraw_traits %&gt;% \n  select(where( ~any(is.na(.)))) %&gt;% \n  summarise(across(everything(), ~sum(is.na(.))))\n#&gt; select: dropped 8 variables (Gradient, Site, PlotID, Individual_nr, ID, …)\n#&gt; summarise: now one row and 2 columns, ungrouped\n#&gt; # A tibble: 1 × 2\n#&gt;    Date Value\n#&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1     7     3\n\nThe variables Date and Value have NAs. It is not always a problem to have missing data. In this case, Date is not a crucial variable, and we know the data was collected during a few days in July 2018. We could just impute one of these dates. But for now, let’s focus on the NAs in Value.\nOnce the missing values are detected one has to decide if the missing data can be recovered, or if the missing values should be removed from the dataset. After checking all the raw data and notes, we cannot find the Values from these observations and have to conclude that the data is useless. So, we want to remove them. For this we will use the function drop_na() on the variable Value.\n\nraw_traits &lt;- raw_traits |&gt; \n  drop_na(Value)\n#&gt; drop_na: removed 3 rows (&lt;1%), 10,294 rows remaining\n\nThis operation has removed 3 rows, which is the number of NA’s in this variable.\nMissing data can also be detected using the validator package. It is however more tedious to write one rule (is.na() or !is.na()) for each variable. But it can also be useful, because the is.na() function can be combined with any() or all(), defining to include/exclude some or all NAs in a variable."
  },
  {
    "objectID": "03_data_curation.html#check-range-of-values",
    "href": "03_data_curation.html#check-range-of-values",
    "title": "4  Data cleaning",
    "section": "\n4.7 Check range of values",
    "text": "4.7 Check range of values\nSome variables might have specific values we want to check. For categorical variables there is usually a list of specific values to test, while for numeric variables, it is more common to have a range of values or upper/lower limits.\n\n4.7.1 Categorical variables\nLet’s look at the variable leaf ID, where we have a list of valid values. For this, we need to get a list of valid IDs, using the get_PFTC_envelope_codes function from the PFTCFunctions package.\n\n#remotes::install_github(\"Plant-Functional-Trait-Course/PFTCFunctions\")\n\nlibrary(\"PFTCFunctions\")\n\nleaf_ID &lt;- get_PFTC_envelope_codes(seed = 32)\n\nLet’s make some rules to check the variables Gradient, Site and leaf_ID.\n\n# rules\nrules &lt;- validator(\n  Gradient %in% c(\"B\", \"C\"),\n  Site %in% c(1:7),\n  ID %in% c(leaf_ID$hashcode))\n\nAnd then check the rules in the dataset.\n\nout &lt;- confront(raw_traits, rules)\nsummary(out)\n#&gt;   name items passes fails nNA error warning                        expression\n#&gt; 1   V1 10294  10294     0   0 FALSE   FALSE        Gradient %vin% c(\"B\", \"C\")\n#&gt; 2   V2 10294  10294     0   0 FALSE   FALSE                 Site %vin% c(1:7)\n#&gt; 3   V3 10294  10280    14   0 FALSE   FALSE ID %vin% c(leaf_ID[[\"hashcode\"]])\n\nThere are 14 rows with a wrong leaf ID.\n\nviolating(raw_traits, out)\n#&gt; # A tibble: 14 × 10\n#&gt;    Date       Gradient  Site PlotID Individual_nr ID      Taxon   Trait    Value\n#&gt;    &lt;date&gt;     &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;\n#&gt;  1 2018-07-21 B            1 A                  2 BGB8422 salix … Plan…   0.7   \n#&gt;  2 2018-07-21 B            1 A                  2 BGB8422 salix … Wet_…   0.0132\n#&gt;  3 2018-07-21 B            1 A                  2 BGB8422 salix … Dry_…   0.0028\n#&gt;  4 2018-07-21 B            1 A                  2 BGB8422 salix … Leaf…   0.267 \n#&gt;  5 2018-07-21 B            1 A                  2 BGB8422 salix … Leaf…   0.538 \n#&gt;  6 2018-07-21 B            1 A                  2 BGB8422 salix … SLA_… 192.    \n#&gt;  7 2018-07-21 B            1 A                  2 BGB8422 salix … LDMC    0.212 \n#&gt;  8 2018-07-21 B            1 A                  2 BGB8422 salix … C_pe…  47.0   \n#&gt;  9 2018-07-21 B            1 A                  2 BGB8422 salix … N_pe…   3.27  \n#&gt; 10 2018-07-21 B            1 A                  2 BGB8422 salix … CN_r…  14.4   \n#&gt; 11 2018-07-21 B            1 A                  2 BGB8422 salix … dN15…   4.41  \n#&gt; 12 2018-07-21 B            1 A                  2 BGB8422 salix … dC13… -31.8   \n#&gt; 13 2018-07-21 B            1 A                  2 BGB8422 salix … P_pe…   0.213 \n#&gt; 14 2018-07-21 B            1 A                  2 BGB8422 salix … NP_r…  15.3   \n#&gt; # ℹ 1 more variable: Elevation_m &lt;dbl&gt;\n\nThe ID BGB8422 does not exist in the list of valid leaf IDs. There might be a typo in this ID. One thing is to search for the letter or number combinations and see if we can figure out where the mistake happened. For this we will use the stringr package, which is part of tidyverse.\nWith the function str_detect() we can search for specific strings like “BGB” or “8422” in the variable hashcode.\n\nleaf_ID |&gt; \n  filter(str_detect(hashcode, \"8422\"))\n#&gt; filter: removed 17,575 rows (&gt;99%), one row remaining\n#&gt; # A tibble: 1 × 1\n#&gt;   hashcode\n#&gt;   &lt;chr&gt;   \n#&gt; 1 BGP8422\n\nWhen searching for the combinations of numbers, we find a ID that is very similar but a B is replaced by the P. This seems like a mistake that could be easily made. The envelope of this sample should also be checked before fixing the error.\n\n\n\n\n\n\nExercise 2\n\n\n\nLet’s fix the wrong leaf ID, by replacing BGB8422 with BGP8422.\nHint\n\nuse if_else()\n\n\n\n\n\n4.7.2 Numeric variables\nFor numeric variables we could use another set of rules. For example the variable LDMC (Leaf dry matter content) is the dry mass divided by the wet mass. If LDMC is larger than 1 it means that the dry leaf was heavier than the wet leaf, which does not make sense. So, a simple rule to test is that LDMC is between 0 and 1.\nFor this you could just use Value &lt;= 1. But because we have different traits, we need to use a conditional rule:\n\n# rules\nrules &lt;- validator(\n  if (Trait == \"LDMC\") Value &lt;= 1)\n\nAnd then check the rules in the dataset.\n\nout &lt;- confront(raw_traits, rules)\nsummary(out)\n#&gt;   name items passes fails nNA error warning\n#&gt; 1   V1 10294  10294     0   0 FALSE   FALSE\n#&gt;                               expression\n#&gt; 1 Trait != \"LDMC\" | (Value - 1 &lt;= 1e-08)\n\nNone of the values is violating the rules, so all is good."
  },
  {
    "objectID": "03_data_curation.html#check-taxonomy",
    "href": "03_data_curation.html#check-taxonomy",
    "title": "4  Data cleaning",
    "section": "\n4.8 Check taxonomy",
    "text": "4.8 Check taxonomy\nA common problem is inconsistencies within variables. In this dataset such a variable is Taxon. It is very common to make mistakes and typos with Latin species names during data entry.\nLet’s look at all unique species names using distinct() and sort them by Taxon using arrange(). This is a good way to see small typos in the species names.\n\nraw_traits |&gt; \n  distinct(Taxon) |&gt; \n  arrange(Taxon) |&gt; \n  print(n = Inf)\n#&gt; distinct: removed 10,259 rows (&gt;99%), 35 rows remaining\n#&gt; # A tibble: 35 × 1\n#&gt;    Taxon                   \n#&gt;    &lt;chr&gt;                   \n#&gt;  1 alopecurus ovatus       \n#&gt;  2 bistorta vivipara       \n#&gt;  3 calalmagrostis neglecta \n#&gt;  4 calamagrostis neglecta  \n#&gt;  5 cassiope tetragona      \n#&gt;  6 cerastium arcticum      \n#&gt;  7 draba arctica           \n#&gt;  8 draba oxycarpa          \n#&gt;  9 dryas octopetala        \n#&gt; 10 equisetum arvense       \n#&gt; 11 equisetum scirpoides    \n#&gt; 12 festuca rubra           \n#&gt; 13 festuca viviparoidea    \n#&gt; 14 luzula confusa          \n#&gt; 15 luzula nivalis          \n#&gt; 16 micranthes hieraciifolia\n#&gt; 17 micranthes nivalis      \n#&gt; 18 oxiria digyna           \n#&gt; 19 oxyra digyna            \n#&gt; 20 oxyria digina           \n#&gt; 21 oxyria digyna           \n#&gt; 22 pedicularis hirsuta     \n#&gt; 23 poa alpina              \n#&gt; 24 poa arctica             \n#&gt; 25 poa pratensis           \n#&gt; 26 potentilla hyparctica   \n#&gt; 27 ranunculus sulphureus   \n#&gt; 28 salix polaris           \n#&gt; 29 saxifraga cernua        \n#&gt; 30 saxifraga cespitosa     \n#&gt; 31 saxifraga hirculus      \n#&gt; 32 saxifraga oppositifolia \n#&gt; 33 silene acaulis          \n#&gt; 34 stellaria longipes      \n#&gt; 35 trisetum spicatum\n\nThere are four different versions for oxyra digyna and two for calamagrostis neglecta. Obviously, some typos where made when entering the data.\n\n4.8.1 Use case_when()\nBecause we have to change multiple species names, we will use case_when(), which allows for multiple conditions.\n\nraw_traits &lt;- raw_traits |&gt; \n  mutate(Taxon = case_when(Taxon %in% c(\"oxiria digyna\", \n                                        \"oxyria digina\", \n                                        \"oxyra digyna\") ~ \"oxyria digyna\",\n                           Taxon == \"calalmagrostis neglecta\" ~ \"calamagrostis neglecta\",\n                           TRUE ~ Taxon))\n#&gt; mutate: changed 38 values (&lt;1%) of 'Taxon' (0 new NAs)\n\n\n4.8.2 Use taxon dictionary\nAn alternative to using case_when() to fix the problem, would be to create a dictionary with bad and good species names.\nLet’s make a taxon dictionary.\n\ndictionary &lt;- tibble(bad_name = c(\"oxiria digyna\", \n                                  \"oxyria digina\", \n                                  \"oxyra digyna\", \n                                  \"calalmagrostis neglecta\"),\n                     good_name = c(\"oxyria digyna\", \n                                   \"oxyria digyna\", \n                                   \"oxyria digyna\", \n                                   \"calamagrostis neglecta\"))\n\nNext, we need to join the dictionary to the dataset using the bad name column. And with coalesce with can replace the bad names with the good names.\n\nraw_traits &lt;- raw_traits |&gt; \n  left_join(dictionary, by = c(\"Taxon\" = \"bad_name\")) |&gt; \n  mutate(Taxon = coalesce(good_name, Taxon))\n  \n\n\n4.8.3 Checking Taxon using TNRS\nIt is always advisable to check the taxonomy using a database such as TNRS, a Taxonomic Name Resolution Service.\nMaitner please add an example using TNRS."
  },
  {
    "objectID": "03_data_curation.html#visualise-data",
    "href": "03_data_curation.html#visualise-data",
    "title": "4  Data cleaning",
    "section": "\n4.9 Visualise data",
    "text": "4.9 Visualise data\nSome errors and problems in the data are difficult to detect by looking at the dataset. For example checking if the measurements are realistic is nearly impossible by going through a table with numbers. For this, visualising the data is much more effective.\n\n4.9.1 Histogram or density plot\nUsing histograms or density plots shows you the range of values in a variable. We are showing the density for each trait and colour the two different gradients.\n\n\n\n\nFigure 4.2: Density distributions of all measured traits.\n\n\n\nNote that the size traits (plant height, leaf mass, area and thickness) have distributions with very long tails. This is common for size related variables and log transformation is common for such variables.\nAlso not that leaf area has a huge tail and goes up to almost 20’000 cm2. This is a leaf of almost 2 m2, which is impossible for a plant from Svalbard. This value needs to be checked, it could be a typo.\nLet’s log transform the size traits first.\n\n\nraw_traits &lt;- raw_traits |&gt; \n  mutate(Value_log = if_else(Trait %in% c(\n    \"Plant_Height_cm\",\n    \"Wet_Mass_g\",\n    \"Dry_Mass_g\",\n    \"Leaf_Area_cm2\",\n    \"Leaf_Thickness_mm\"), log(Value), Value),\n    Trait = recode(Trait,\n                   \"Plant_Height_cm\" = \"Plant_Height_cm_log\",\n                   \"Wet_Mass_g\" = \"Wet_Mass_g_log\",\n                   \"Dry_Mass_g\" = \"Dry_Mass_g_log\",\n                   \"Leaf_Area_cm2\" = \"Leaf_Area_cm2_log\",\n                   \"Leaf_Thickness_mm\" = \"Thickness_mm_log\"))\n#&gt; Warning: There was 1 warning in `.fun()`.\n#&gt; ℹ In argument: `Value_log = if_else(...)`.\n#&gt; Caused by warning in `log()`:\n#&gt; ! NaNs produced\n#&gt; mutate: changed 5,214 values (51%) of 'Trait' (0 new NAs)\n#&gt;         new variable 'Value_log' (double) with 7,990 unique values and 0% NA\n\nAnd remake the density plot using the log-transformed values.\n\n\n\n\nFigure 4.3: Density distributions of all measured traits.\n\n\n\nThe size traits do not have a long tail anymore.\nLet’s find the giant leaf. For this we can filter observations in the trait Leaf Area that have Value larger than 10.\n\nraw_traits |&gt; \n  filter(Trait == \"Leaf_Area_cm2_log\",\n         Value &gt; 10)\n#&gt; filter: removed 10,293 rows (&gt;99%), one row remaining\n#&gt; # A tibble: 1 × 11\n#&gt;   Date       Gradient  Site PlotID Individual_nr ID      Taxon       Trait Value\n#&gt;   &lt;date&gt;     &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1 2018-07-18 C            5 C                  2 ANH3472 oxyria dig… Leaf… 17965\n#&gt; # ℹ 2 more variables: Elevation_m &lt;dbl&gt;, Value_log &lt;dbl&gt;\n\nThis value for Leaf Area is impossible. We can again check the envelope of this leaf to find out if this was a typo. It turns out the comma was missed when typing in the data. Let’s fix the problem with a mutate and if_else() statement. Note that we have to fix the problem for the Value and Value_log column.\n\nraw_traits &lt;- raw_traits |&gt; \n  mutate(Value = if_else(ID == \"ANH3472\" & Trait == \"Leaf_Area_cm2_log\", 1.7965, Value),\n         Value_log = if_else(ID == \"ANH3472\" & Trait == \"Leaf_Area_cm2_log\", log(1.7965), Value_log))\n#&gt; mutate: changed one value (&lt;1%) of 'Value' (0 new NAs)\n#&gt;         changed one value (&lt;1%) of 'Value_log' (0 new NAs)\n\n\n4.9.2 Correlations\nAnother way to check the data is to plot variables against each other that should be correlated. In this dataset, we can plot dry mass against leaf area. We would expect a positive correlation between the two variables, where large leaves have a higher dry mass.\n\n#&gt; Warning: Removed 39 rows containing missing values or values outside the scale range\n#&gt; (`geom_point()`).\n\n\n\nFigure 4.4: Correlation between leaf area and dry mass.\n\n\n\nWe see a good correlation between leaf area and dry mass. However, there is a cloud with observations that separate from the main data cloud. These leaves have a lower dry mass and area.\n\n\n\n\n\n\nExercise 3\n\n\n\nWhat could the problem be?\nHint\n\nUse filter() to look at the observations that have a higher leaf area and mass and compare them to the rest of the data.\nUnits\n\n\n\nThe problem with the separate data points is the unit. They were measured in mg instead of g and are 3 digits off. This can easily be seen, when adding the regression lines for each data cloud, and two lines in between with the same slope but the intercept being 10 times smaller.\n\n#&gt; Warning: Removed 39 rows containing missing values or values outside the scale range\n#&gt; (`geom_point()`).\n\n\n\nFigure 4.5: Correlation between leaf area and dry mass."
  }
]